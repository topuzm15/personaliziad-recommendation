{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csc_matrix\n",
    "from dask import dataframe as dd\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    path = 'data/ensemble_train/'\n",
    "    model_data_path = 'model_data/collabrative/ensemble_train/ml/'\n",
    "else:\n",
    "    path = 'data/ensemble/'\n",
    "    model_data_path = 'model_data/collabrative/ensemble/ml/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_pickle(path+'transactions.pkl')[[\"customer_id\", \"article_id\"]]\n",
    "customer_purchase_number = transactions.groupby(\"customer_id\").size().to_frame(\"prod_number\").reset_index()\n",
    "transactions = transactions.merge(customer_purchase_number, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "c_ids = transactions.customer_id.unique()\n",
    "number_of_customer = len(c_ids)\n",
    "customer_encoding = {c_id: i for i, c_id in enumerate(c_ids)}\n",
    "p_ids = transactions.article_id.unique()\n",
    "number_of_products = len(p_ids)\n",
    "product_encoding = {p_id: i for i, p_id in enumerate(p_ids)}\n",
    "transactions.customer_id = transactions.customer_id.map(customer_encoding)\n",
    "transactions.article_id = transactions.article_id.map(product_encoding)\n",
    "p_ids = list(product_encoding.values())\n",
    "\n",
    "with open(model_data_path+'customer_id_encoding.json', 'w') as fp:\n",
    "    json.dump(customer_encoding, fp)\n",
    "\n",
    "with open(model_data_path+'product_id_encoding.json', 'w') as fp:\n",
    "    json.dump(product_encoding, fp)\n",
    "\n",
    "del c_ids, customer_purchase_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(684744, 9857)\n"
     ]
    }
   ],
   "source": [
    "sparseMatrix = csc_matrix((len(customer_encoding.keys()), len(product_encoding.keys())), dtype = np.int8).toarray()\n",
    "data = transactions.groupby([\"customer_id\"])\n",
    "for customer_id, customer_data in data:\n",
    "    for i in customer_data.article_id.unique():\n",
    "        sparseMatrix[customer_id, i] = 1\n",
    "print(sparseMatrix.shape)\n",
    "del product_encoding, customer_encoding, data, transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF( n_components=128,\n",
    "             init='random',\n",
    "             random_state=43,\n",
    "             max_iter=500\n",
    "            )\n",
    "W = model.fit_transform(sparseMatrix)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_data_path+'nmf_model_W.pkl', 'wb') as f:\n",
    "    pickle.dump(W,f)\n",
    "\n",
    "with open(model_data_path+'nmf_model_H.pkl', 'wb') as f:\n",
    "    pickle.dump(H.T,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentDirichletAllocation(\n",
    "    n_components=128,\n",
    "    random_state=43,\n",
    "    max_iter=500\n",
    ")\n",
    "W = model.fit_transform(sparseMatrix)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_data_path+'lda_model_W.pkl', 'wb') as f:\n",
    "    pickle.dump(W,f)\n",
    "\n",
    "with open(model_data_path+'lda_model_H.pkl', 'wb') as f:\n",
    "    pickle.dump(H.T,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_data_path+'customer_id_encoding.json', 'r') as fp:\n",
    "    customer_encoding = json.load(fp)\n",
    "\n",
    "with open(model_data_path+'product_id_encoding.json', 'r') as fp:\n",
    "    product_encoding = json.load(fp)\n",
    "    product_encoding = {int(k): v for k,v in product_encoding.items()}\n",
    "\n",
    "customers = pd.read_pickle(path+\"customers.pkl\")\n",
    "customers.drop_duplicates(subset=[\"customer_id\"], inplace=True)\n",
    "customers.sort_values(by=\"customer_id\", inplace=True)\n",
    "customer_ids = customers[\"customer_id\"].map(customer_encoding).values\n",
    "articles = pd.read_pickle(path+\"articles.pkl\")\n",
    "articles.drop_duplicates(subset=[\"article_id\"], inplace=True)\n",
    "article_ids = articles[\"article_id\"].map(product_encoding).tolist()\n",
    "\n",
    "\n",
    "with open(model_data_path + 'nmf_model_W.pkl', 'rb') as f:\n",
    "    customers = pickle.load(f)[customer_ids]\n",
    "    customers = tf.convert_to_tensor(customers, dtype=tf.float32)\n",
    "\n",
    "with open(model_data_path + 'nmf_model_H.pkl', 'rb') as f:\n",
    "    products = pickle.load(f)[article_ids]\n",
    "    products = tf.convert_to_tensor(customers, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684544: %99.97"
     ]
    }
   ],
   "source": [
    "batch = 512\n",
    "step = 15000 // batch\n",
    "f.close()\n",
    "f = h5py.File('model_data/collabrative/nmf.h5', 'w', libver='latest')\n",
    "dset = f.create_dataset(\"nmf\", (customers.shape[0], 2, 150), dtype=np.float32, compression='gzip')\n",
    "\n",
    "ptr = 0\n",
    "score_temp = []\n",
    "indices_temp = []\n",
    "for i, batch_i in enumerate(range(0, customers.shape[0], batch)):\n",
    "    customer_batch = tf.nn.l2_normalize(customers[batch_i:batch_i+batch], 1)\n",
    "    batch_distances = tf.matmul(customer_batch, products, transpose_b=True)\n",
    "\n",
    "    for distance in batch_distances:\n",
    "        values, indices = tf.math.top_k(distance, k=150)\n",
    "        score_temp.append(values)\n",
    "        indices_temp.append(tf.cast(indices, tf.float32))\n",
    "\n",
    "    if i != 0 and (i % step == 0 or i == (customers.shape[0] // batch)):\n",
    "        score_temp = np.asarray(score_temp, dtype=np.float32)\n",
    "        indices_temp = np.asarray(indices_temp, dtype=np.float32)\n",
    "        dset[ptr:ptr+score_temp.shape[0],1,:] = score_temp\n",
    "        dset[ptr:ptr+indices_temp.shape[0],0,:] = indices_temp\n",
    "        ptr = batch_i+batch\n",
    "        score_temp = []\n",
    "        indices_temp = []\n",
    "\n",
    "    print('\\r' + f'{i*batch}: %{round(100*i*batch/customers.shape[0], 2)}', end='')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_data_path + 'lda_model_W.pkl', 'rb') as f:\n",
    "    customers = pickle.load(f)[customer_ids]\n",
    "    customers = tf.convert_to_tensor(customers, dtype=tf.float32)\n",
    "\n",
    "with open(model_data_path + 'lda_model_H.pkl', 'rb') as f:\n",
    "    products = pickle.load(f)[article_ids]\n",
    "    products = tf.convert_to_tensor(customers, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684544: %99.97"
     ]
    }
   ],
   "source": [
    "batch = 512\n",
    "step = 15000 // batch\n",
    "f.close()\n",
    "f = h5py.File('model_data/collabrative/lda.h5', 'w', libver='latest')\n",
    "dset = f.create_dataset(\"lda\", (customers.shape[0], 2, 150), dtype=np.float32, compression='gzip')\n",
    "\n",
    "ptr = 0\n",
    "score_temp = []\n",
    "indices_temp = []\n",
    "for i, batch_i in enumerate(range(0, customers.shape[0], batch)):\n",
    "    customer_batch = tf.nn.l2_normalize(customers[batch_i:batch_i+batch], 1)\n",
    "    batch_distances = tf.matmul(customer_batch, products, transpose_b=True)\n",
    "\n",
    "    for distance in batch_distances:\n",
    "        values, indices = tf.math.top_k(distance, k=150)\n",
    "        score_temp.append(values)\n",
    "        indices_temp.append(tf.cast(indices, tf.float32))\n",
    "\n",
    "    if i != 0 and (i % step == 0 or i == (customers.shape[0] // batch)):\n",
    "        score_temp = np.asarray(score_temp, dtype=np.float32)\n",
    "        indices_temp = np.asarray(indices_temp, dtype=np.float32)\n",
    "        dset[ptr:ptr+score_temp.shape[0],1,:] = score_temp\n",
    "        dset[ptr:ptr+indices_temp.shape[0],0,:] = indices_temp\n",
    "        ptr = batch_i+batch\n",
    "        score_temp = []\n",
    "        indices_temp = []\n",
    "\n",
    "    print('\\r' + f'{i*batch}: %{round(100*i*batch/customers.shape[0], 2)}', end='')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0286e36f0b6cc42564d593afe30408db71b090a133f6805167cf9f46adee6ce3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
