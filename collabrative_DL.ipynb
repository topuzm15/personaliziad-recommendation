{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "from dask import dataframe as dd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    path = 'data/ensemble_train/'\n",
    "    model_data_path = 'model_data/collabrative/ensemble_train/dl/'\n",
    "    result_path = \"personalization/ensemble_train/\"\n",
    "else:\n",
    "    path = 'data/ensemble/'\n",
    "    model_data_path = 'model_data/collabrative/ensemble/dl/'\n",
    "    result_path = \"personalization/ensemble/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_pickle(path+'transactions.pkl')[[\"customer_id\", \"article_id\"]]\n",
    "customer_purchase_number = transactions.groupby(\"customer_id\").size().to_frame(\"prod_number\").reset_index()\n",
    "transactions = transactions.merge(customer_purchase_number, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "c_ids = transactions.customer_id.unique()\n",
    "number_of_customer = len(c_ids)\n",
    "customer_encoding = {str(c_id): i for i, c_id in enumerate(c_ids)}\n",
    "p_ids = transactions.article_id.unique()\n",
    "number_of_products = len(p_ids)\n",
    "product_encoding = {int(p_id): i for i, p_id in enumerate(p_ids)}\n",
    "with open(model_data_path+'customer_id_encoding.json', 'w') as fp:\n",
    "    json.dump(customer_encoding, fp)\n",
    "\n",
    "with open(model_data_path+'product_id_encoding.json', 'w') as fp:\n",
    "    json.dump(product_encoding, fp)\n",
    "\n",
    "transactions.customer_id = transactions.customer_id.map(customer_encoding)\n",
    "transactions.article_id = transactions.article_id.map(product_encoding)\n",
    "p_ids = list(product_encoding.values())\n",
    "del c_ids, product_encoding, customer_encoding, customer_purchase_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train True: customer num: 684744 | prod num: 9857 \n",
    "Train False: customer num: 684649 | prod num: 9828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(transactions, p_ids):\n",
    "    transactions = transactions.groupby([\"customer_id\"])['article_id']\\\n",
    "                                .apply(lambda x: list(x), meta=(\"article_ids\",object))\\\n",
    "                                .reset_index().compute().drop_duplicates(subset=[\"customer_id\"])\n",
    "\n",
    "    transactions[\"hist_len\"] = transactions.article_ids.apply(lambda x: 128 if len(x)>128 else len(x))\n",
    "    transactions[\"prod_ids\"] = transactions.apply(lambda x: random.sample(x.article_ids, x.hist_len), axis=1)\n",
    "    transactions[\"not_prods\"] = transactions.prod_ids.apply(lambda x: [p_id for p_id in random.sample(p_ids, 256) if p_id not in x][:128])\n",
    "\n",
    "    return transactions[[\"customer_id\", \"prod_ids\", \"not_prods\"]]\n",
    "\n",
    "transactions = prepare_data(dd.from_pandas(transactions, npartitions=4), p_ids)\n",
    "transactions.to_pickle(model_data_path+'train.pkl')\n",
    "del p_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, positive_sample_length, negative_sample_length, batch_size):\n",
    "        self.data = data\n",
    "        self.positive_sample_length = positive_sample_length\n",
    "        self.negative_sample_length = negative_sample_length\n",
    "        self.sample_length = positive_sample_length + negative_sample_length\n",
    "        self.batch_size = batch_size\n",
    "        if batch_size % self.sample_length != 0:\n",
    "            raise ValueError(\"batch_size must be divisible by sum of positive_sample_length and negative_sample_length\")\n",
    "\n",
    "\n",
    "    def user_info_generator(self):\n",
    "        for i, row in self.data[[\"customer_id\",\"prod_ids\",\"not_prods\"]].iterrows():\n",
    "            if len(row.prod_ids) >= self.positive_sample_length:\n",
    "                pids = np.asarray(random.sample(row[\"prod_ids\"], self.positive_sample_length) + random.sample(row[\"not_prods\"], self.negative_sample_length))\n",
    "            else:\n",
    "                pids = np.asarray(random.sample(row[\"prod_ids\"],1) + random.sample(row[\"not_prods\"], self.sample_length -1))\n",
    "            labels = np.asarray([1]*self.positive_sample_length + [0]*self.negative_sample_length)\n",
    "            indices = np.arange(self.sample_length)\n",
    "            np.random.shuffle(indices)\n",
    "            yield tf.convert_to_tensor(np.asarray([pids[indices],labels[indices]], dtype=np.int32), dtype=tf.int32)\n",
    "\n",
    "    def user_id_generator(self):\n",
    "        for i, row in self.data[[\"customer_id\"]].iterrows():\n",
    "            customer_id = [int(row[\"customer_id\"])] * self.sample_length\n",
    "            yield tf.convert_to_tensor(customer_id, dtype=tf.int32)\n",
    "\n",
    "class BatchGenerator(Generator):\n",
    "    def __init__(self, data, positive_sample_length, negative_sample_length, batch_size):\n",
    "        super().__init__(data, positive_sample_length, negative_sample_length, batch_size)\n",
    "        self.mini_batch = int(batch_size / self.sample_length)\n",
    "        self.batch = batch_size\n",
    "\n",
    "        self.user_info_loader = tf.data.Dataset.from_generator(\n",
    "        self.user_info_generator, output_types=tf.int32).batch(self.mini_batch, drop_remainder=True)\n",
    "        self.user_id_loader = tf.data.Dataset.from_generator(\n",
    "        self.user_id_generator, output_types=tf.int32).batch(self.mini_batch, drop_remainder=True)\n",
    "\n",
    "    def _get_batch(self):\n",
    "        for c_ids, info in zip(self.user_id_loader, self.user_info_loader):\n",
    "            x = tf.stack([tf.reshape(c_ids, self.batch), tf.reshape(info[:,0,:], self.batch)], axis=1)\n",
    "            y = tf.reshape(info[:,1,:], self.batch)\n",
    "            yield x, y\n",
    "\n",
    "class DataGenerator (BatchGenerator):\n",
    "    def __init__(self, data, positive_sample_length, negative_sample_length, batch_size, validation=False):\n",
    "        super().__init__(data, positive_sample_length, negative_sample_length, batch_size)\n",
    "        self.len_data = len(self.data) // self.batch_size\n",
    "        self.validation = validation\n",
    "        if validation:\n",
    "            self.val_data = self.data.copy()\n",
    "            self.data = self.val_data.sample(frac=0.1)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.validation:\n",
    "            self.data = self.val_data.sample(frac=0.1)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(iter(self._get_batch()))\n",
    "\n",
    "    def get(self):\n",
    "        return self._get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(model_data_path+\"train.pkl\")\n",
    "train_data = dd.from_dataframe(train_data, npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_generator = DataGenerator(train_data, 3, 5, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization Model (GMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmfNet(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_prods, embedding_size, **kwargs):\n",
    "        super(GmfNet, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_prods = num_prods\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = tf.keras.layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer= tf.keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.user_bias = tf.keras.layers.Embedding(num_users, 1)\n",
    "        self.prod_embedding = tf.keras.layers.Embedding(\n",
    "            num_prods,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer= tf.keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.prod_bias = tf.keras.layers.Embedding(num_prods, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        user_bias = self.user_bias(inputs[:, 0])\n",
    "        prod_vector = self.prod_embedding(inputs[:, 1])\n",
    "        prod_bias = self.prod_bias(inputs[:, 1])\n",
    "        dot_user_prod = tf.tensordot(user_vector, prod_vector, 2)\n",
    "        x = dot_user_prod + user_bias + prod_bias\n",
    "\n",
    "        return tf.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpNet(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_prods, embedding_size, **kwargs):\n",
    "        super(MlpNet, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_prods = num_prods\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = tf.keras.layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer= tf.keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.prod_embedding = tf.keras.layers.Embedding(\n",
    "            num_prods,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer= tf.keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "\n",
    "        self.prediction = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dropout(0.1),\n",
    "            tf.keras.layers.Dense(embedding_size, activation=\"relu\", name=\"layer1\"),\n",
    "            tf.keras.layers.Dropout(0.1),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(embedding_size, activation=\"relu\", name=\"layer2\"),\n",
    "            tf.keras.layers.Dropout(0.1),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(embedding_size, activation=\"relu\", name=\"layer3\"),\n",
    "            tf.keras.layers.Dense(embedding_size, activation=\"relu\", name=\"layer4\"),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        prod_vector = self.prod_embedding(inputs[:, 1])\n",
    "\n",
    "        return self.prediction(tf.concat([user_vector, prod_vector], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderNet(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_prods, embedding_size, **kwargs):\n",
    "        super(RecommenderNet, self).__init__(**kwargs)\n",
    "        self.gmf = GmfNet(num_users, num_prods, embedding_size)\n",
    "        self.mlp = MlpNet(num_users, num_prods, embedding_size)\n",
    "        self.pred = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        gmf_output = self.gmf(inputs)\n",
    "        mlp_output = self.mlp(inputs)\n",
    "        return self.pred(tf.concat([gmf_output, mlp_output], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = RecommenderNet(number_of_customer, c, 128) #1362281 104547\n",
    "model = RecommenderNet(1362281, 104547, 128)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\"accuracy\"])\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"weihts\",\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "cores = multiprocessing.cpu_count()\n",
    "history = model.fit(train_generator, epochs=25, callbacks=[es, checkpoint], workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_data_path + 'customer_id_encoding.json', 'r') as fp:\n",
    "    customer_encoding = json.load(fp)\n",
    "\n",
    "with open(model_data_path + 'product_id_encoding.json', 'r') as fp:\n",
    "    product_encoding = json.load(fp)\n",
    "    product_encoding = {int(k): v for k,v in product_encoding.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_pickle(path+\"customers.pkl\")\n",
    "customers.drop_duplicates(subset=[\"customer_id\"], inplace=True)\n",
    "customers.sort_values(by=\"customer_id\", inplace=True)\n",
    "customers = customers[\"customer_id\"].map(customer_encoding).values\n",
    "articles = pd.read_pickle(path+\"articles.pkl\")\n",
    "articles.drop_duplicates(subset=[\"article_id\"], inplace=True)\n",
    "articles = articles[\"article_id\"].map(product_encoding).tolist()\n",
    "number_of_articles = len(articles)\n",
    "number_of_customer = len(customers)\n",
    "batch_size = 128\n",
    "articles = articles * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecommenderNet(len(customers), len(articles) // batch_size, 128)\n",
    "model.build(input_shape=(None, 2))\n",
    "model.load_weights(\"weights/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684544: %99.98"
     ]
    }
   ],
   "source": [
    "step = 50000 // batch_size\n",
    "f = h5py.File(result_path+'dl.h5', 'w', libver='latest')\n",
    "dset = f.create_dataset(\"dl\", (number_of_customer, number_of_articles), dtype=np.float32, compression='gzip')\n",
    "\n",
    "ptr=0\n",
    "temp = []\n",
    "loop_size = number_of_customer + batch_size\n",
    "for i,batch_i in enumerate(range(batch_size, loop_size, batch_size)):\n",
    "    customer_ids_batch = customers[batch_i-batch_size:batch_i]\n",
    "    customer_ids_batch = np.repeat(customer_ids_batch, len(articles)/batch_size)\n",
    "    inputs = np.vstack((customer_ids_batch, articles[:customer_ids_batch.shape[0]])).T\n",
    "    tf_inputs = tf.convert_to_tensor(inputs, dtype=tf.int32)\n",
    "\n",
    "    temp += model(tf_inputs).numpy().reshape(-1).tolist()\n",
    "    if i != 0 and (i % step == 0 or i == (number_of_customer // batch_size)):\n",
    "        temp = np.asarray(temp, dtype=np.float32).reshape(len(temp)//number_of_articles, number_of_articles)\n",
    "        dset[ptr:ptr+temp.shape[0],:] = temp\n",
    "        ptr = (i+1) * batch_size\n",
    "        temp = []\n",
    "    print('\\r' + f'{i*batch_size}: %{round(100*i*batch_size/number_of_customer, 2)}', end='')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0286e36f0b6cc42564d593afe30408db71b090a133f6805167cf9f46adee6ce3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
