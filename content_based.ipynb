{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import dask\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import dask.array as da\n",
    "import tensorflow as tf\n",
    "from dask import dataframe as dd\n",
    "from scipy import sparse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from dask_ml.wrappers import Incremental\n",
    "from dask_ml.decomposition import TruncatedSVD\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Based Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    path = 'data/ensemble_train/'\n",
    "    model_path = \"d2v_train.model\"\n",
    "else:\n",
    "    path = 'data/ensemble/'\n",
    "    model_path = \"d2v.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(path+\"corpus.csv\")\n",
    "indexes = corpus[corpus.type == \"product\"].index.tolist()\n",
    "ids = corpus[corpus.type == \"product\"].doc_id.tolist()\n",
    "id_dict = dict(zip(ids, indexes))\n",
    "with open('model_data/content/prod_id_dict.json', 'w') as fp:\n",
    "    json.dump(id_dict, fp)\n",
    "indexes = corpus[corpus.type == \"customer\"].index.tolist()\n",
    "ids = corpus[corpus.type == \"customer\"].doc_id.tolist()\n",
    "id_dict = dict(zip(ids, indexes))\n",
    "with open('model_data/content/customer_id_dict.json', 'w') as fp:\n",
    "    json.dump(id_dict, fp)\n",
    "del corpus, indexes, ids, id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s9/93b6jyx57x33y6tt142gsgjc0000gn/T/ipykernel_77850/3226832552.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"corpus.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"doc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mDocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_d\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m     \"\"\"\n\u001b[1;32m   1422\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv(path+\"corpus.csv\", index_col=False)[\"doc\"].apply(lambda x: x.replace('.','')).to_list()\n",
    "Documents = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec\n",
    "epochs = 25\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "model= Doc2Vec(dm=0,\n",
    "               vector_size=512,\n",
    "               negative=5,\n",
    "               hs=0,\n",
    "               min_count=2,\n",
    "               sample = 0,\n",
    "               workers=cores)\n",
    "\n",
    "model.build_vocab(Documents)\n",
    "for epoch in range(epochs):\n",
    "    print('iteration {0}'.format(epoch),end = \"\\r\")\n",
    "    model.train(Documents,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=1)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"weights/\"+model_path)\n",
    "print(\"Model Saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"weights/d2v.model\")\n",
    "model_vectors = model.dv.get_normed_vectors()\n",
    "articles = pd.read_pickle(\"data/articles.pkl\")\n",
    "customers = pd.read_pickle(\"data/articles.pkl\")\n",
    "with open(\"model_data/prod_id_dict.json\",\"r\") as f:\n",
    "    prod_dict = json.load(f)\n",
    "\n",
    "    prod_ids = list(json.load(f).values())\n",
    "\n",
    "\n",
    "prod_d2v_gpu = tf.nn.l2_normalize(tf.convert_to_tensor(model_vectors[prod_ids]))\n",
    "customer_d2v = tf.convert_to_tensor(np.delete(model_vectors, prod_ids,axis=0))\n",
    "del model, model_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 512\n",
    "step = 15000 // batch\n",
    "f = h5py.File('model_data/content/d2v.h5', 'w', libver='latest')\n",
    "dset = f.create_dataset(\"d2v\", (customer_d2v.shape[0], 2, 150), dtype=np.float32, compression='gzip')\n",
    "\n",
    "ptr = 0\n",
    "score_temp = []\n",
    "indices_temp = []\n",
    "for i, batch_i in enumerate(range(0, customer_d2v.shape[0], batch)):\n",
    "    customer_batch = tf.nn.l2_normalize(customer_d2v[batch_i:batch_i+batch], 1)\n",
    "    batch_distances = tf.matmul(customer_batch, prod_d2v_gpu, transpose_b=True)\n",
    "    for distance in batch_distances:\n",
    "        values, indices = tf.math.top_k(distance, k=150)\n",
    "        score_temp.append(values)\n",
    "        indices_temp.append(tf.cast(indices, tf.float32))\n",
    "\n",
    "    if i != 0 and (i % step == 0 or i == (customer_d2v.shape[0] // batch)):\n",
    "        score_temp = np.asarray(score_temp, dtype=np.float32)\n",
    "        indices_temp = np.asarray(indices_temp, dtype=np.float32)\n",
    "        dset[ptr:ptr+score_temp.shape[0],1,:] = score_temp\n",
    "        dset[ptr:ptr+indices_temp.shape[0],0,:] = indices_temp\n",
    "        ptr = batch_i+batch\n",
    "        score_temp = []\n",
    "        indices_temp = []\n",
    "\n",
    "    print('\\r' + f'{i*batch}: %{round(100*i*batch/customer_d2v.shape[0], 2)}', end='')\n",
    "f.close()\n",
    "del customer_d2v, prod_d2v_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content based TfIdf results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf word level implementation //its run about 11 minutes\n",
    "corpus = pd.read_csv('data/corpus.csv', index_col=False)[\"doc\"].apply(lambda x: x.replace('.','')).to_list()\n",
    "vectorizer = TfidfVectorizer(lowercase=True,max_features=8192, dtype=np.float32)\n",
    "model_vectors = vectorizer.fit_transform(corpus)\n",
    "\n",
    "del corpus\n",
    "with open(\"model_data/prod_id_dict.json\",\"r\") as f:\n",
    "    prod_ids = list(json.load(f).values())\n",
    "    prod_tf_gpu = tf.convert_to_tensor(model_vectors[prod_ids].toarray())\n",
    "    del prod_ids\n",
    "with open(\"model_data/customer_id_dict.json\",\"r\") as f:\n",
    "    customer_ids = list(json.load(f).values())\n",
    "    customer_tf = model_vectors[customer_ids].toarray()\n",
    "    del customer_ids, model_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 512\n",
    "step = 15000 // batch\n",
    "f.close()\n",
    "f = h5py.File('model_data/content/tf_idf.h5', 'w', libver='latest')\n",
    "dset = f.create_dataset(\"tf_idf\", (customer_tf.shape[0], 2, 150), dtype=np.float32, compression='gzip')\n",
    "\n",
    "ptr = 0\n",
    "score_temp = []\n",
    "indices_temp = []\n",
    "for i, batch_i in enumerate(range(0, customer_tf.shape[0], batch)):\n",
    "    customer_batch = tf.convert_to_tensor(customer_tf[batch_i:batch_i+batch])\n",
    "    batch_distances = tf.matmul(customer_batch, prod_tf_gpu, transpose_b=True)\n",
    "    for distance in batch_distances:\n",
    "        values, indices = tf.math.top_k(distance, k=150)\n",
    "        score_temp.append(values)\n",
    "        indices_temp.append(tf.cast(indices, tf.float32))\n",
    "\n",
    "    if i != 0 and (i % step == 0 or i == (customer_tf.shape[0] // batch)):\n",
    "        score_temp = np.asarray(score_temp, dtype=np.float32)\n",
    "        indices_temp = np.asarray(indices_temp, dtype=np.float32)\n",
    "        dset[ptr:ptr+score_temp.shape[0],1,:] = score_temp\n",
    "        dset[ptr:ptr+indices_temp.shape[0],0,:] = indices_temp\n",
    "        ptr = batch_i+batch\n",
    "        score_temp = []\n",
    "        indices_temp = []\n",
    "    print('\\r' + f'{i*batch}: %{round(100*i*batch/customer_tf.shape[0], 2)}', end='')\n",
    "f.close()\n",
    "del customer_tf, prod_tf_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Content Based Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = h5py.File('model_data/content/tf_idf.h5', 'r')[\"tf_idf\"]\n",
    "submission = pd.read_csv('data/sample_submission.csv', index_col=False)\n",
    "with open(\"model_data/customer_id_dict.json\",\"r\") as f:\n",
    "    customer_ids = json.load(f)\n",
    "    customer_ids = {k:v-105542 for k,v in customer_ids.items()}\n",
    "with open(\"model_data/prod_id_dict.json\",\"r\") as f:\n",
    "    prod_ids = json.load(f)\n",
    "    prod_ids = {v:k for k,v in prod_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "results = np.asanyarray(results)\n",
    "for i, row in submission.iterrows():\n",
    "    if row.customer_id in customer_ids:\n",
    "        prods = results[customer_ids[row.customer_id],0]\n",
    "        predictions.append(' '.join([prod_ids[int(i)] for i in prods[:12]]))\n",
    "    else:\n",
    "        predictions.append(row.prediction)\n",
    "    print('\\r' + f'{i}: %{round(100*i/submission.shape[0], 2)}', end='')\n",
    "    \n",
    "submission[\"prediction\"] = predictions\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = h5py.File('model_data/content/d2v.h5', 'r')[\"d2v\"]\n",
    "submission = pd.read_csv('data/sample_submission.csv', index_col=False)\n",
    "with open(\"model_data/customer_id_dict.json\",\"r\") as f:\n",
    "    customer_ids = json.load(f)\n",
    "    customer_ids = {k:v-105542 for k,v in customer_ids.items()}\n",
    "with open(\"model_data/prod_id_dict.json\",\"r\") as f:\n",
    "    prod_ids = json.load(f)\n",
    "    prod_ids = {v:k for k,v in prod_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "results = np.asanyarray(results)\n",
    "for i, row in submission.iterrows():\n",
    "    if row.customer_id in customer_ids:\n",
    "        prods = results[customer_ids[row.customer_id],0]\n",
    "        predictions.append(' '.join([prod_ids[int(i)] for i in prods[:12]]))\n",
    "    else:\n",
    "        predictions.append(row.prediction)\n",
    "    print('\\r' + f'{i}: %{round(100*i/submission.shape[0], 2)}', end='')\n",
    "    \n",
    "submission[\"prediction\"] = predictions\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad61a90f8b8ec2120f8b8d3efc4267caee6017c89f89db236bf61b71644f2c7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
