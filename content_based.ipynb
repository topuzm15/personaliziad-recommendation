{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import dask\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import dask.array as da\n",
    "import tensorflow as tf\n",
    "from dask import dataframe as dd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from dask_ml.wrappers import Incremental\n",
    "from dask_ml.decomposition import TruncatedSVD\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = dd.read_csv('data/articles.csv')\n",
    "transactions = dd.read_csv('data/transactions_train.csv')\n",
    "customers = dd.read_csv('data/customers.csv')\n",
    "train, test = transactions.random_split([0.9, 0.1], random_state=43)\n",
    "df = test.merge(train[[\"customer_id\"]], on=[\"customer_id\"], how=\"outer\", indicator=True)\n",
    "train = dd.concat([train, df[df['_merge'] == 'left_only'][[\"customer_id\", \"article_id\"]]], axis=0, ignore_index=True, interleave_partitions=True, ignore_order=True)\n",
    "df = df[df['_merge'] == 'both'][[\"customer_id\"]].drop_duplicates()\n",
    "test = test.merge(df, how=\"inner\", on=\"customer_id\")\n",
    "map_test = test.groupby([\"customer_id\"])['article_id'].apply(lambda x: list(x), meta=(\"article_ids\",object)).reset_index().compute().drop_duplicates(subset=[\"customer_id\"])\n",
    "print(\"Len of data: \", len(transactions))\n",
    "del transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling NONE values\n",
    "customers.FN = customers.FN.fillna(0)\n",
    "customers.Active = customers.Active.fillna(0)\n",
    "customers.age = customers.age.fillna(customers.age.mean())\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.fillna(\"not_regular\")\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.apply(lambda x: \"not_regular\" if x == \"NONE\" or x == \"None\" else x, meta=('fashion_news_frequency', 'object'))\n",
    "train.price = train.price.fillna(train.price.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statical information extraction\n",
    "prod_count = train.groupby(\"customer_id\").agg({\"customer_id\":\"count\"}).rename(columns={\"customer_id\":\"bought_\"}).reset_index()\n",
    "customers = customers.merge(prod_count, on=\"customer_id\", how=\"inner\")\n",
    "prod_price = train.groupby(\"customer_id\").agg({\"price\":[\"mean\", \"std\"]}).rename(columns={\"customer_id\":\"price_\"}).reset_index()\n",
    "prod_price.columns = list(map(''.join, prod_price.columns.values))\n",
    "customers = customers.merge(prod_price, on=\"customer_id\", how=\"inner\")\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating customer last 64 order history\n",
    "def get_purchase_history(x):\n",
    "    purchase_history = zip(x[\"t_dat\"], x[\"article_id\"])\n",
    "    purchase_history_in_order = sorted(purchase_history, key=lambda i: i[0],reverse=True)[:64]\n",
    "    return [i[1] for i in purchase_history_in_order]\n",
    "\n",
    "\n",
    "purchase_history = train.groupby([\"customer_id\"]).apply(get_purchase_history, meta=(\"article_ids\",object)).reset_index().compute().drop_duplicates(\"customer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting articles to document\n",
    "articles = dd.read_csv('data/articles.csv')\n",
    "def article2doc(x):\n",
    "    def clean_doc(text):\n",
    "        unwanted_chars = ['1','2','3','4','5','6','7','8','9','(',')','[',']']\n",
    "        for chr in unwanted_chars:\n",
    "            text = text.replace(chr, '')\n",
    "        return text\n",
    "\n",
    "    doc =  '. '.join([x.prod_name, x.product_type_name, x.product_group_name, x.graphical_appearance_name, x.colour_group_name,\\\n",
    "                      x.perceived_colour_value_name, x.perceived_colour_master_name, x.department_name, x.index_name, x.index_group_name,\\\n",
    "                      x.section_name, x.garment_group_name, str(x.detail_desc)])[:-1]\n",
    "    return(clean_doc(doc))\n",
    "\n",
    "articles[\"doc\"] = articles.apply(article2doc, axis=1, meta=(\"doc\",\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting customer purchase histroy and product features to document\n",
    "prod_dict = {}\n",
    "for i,row in articles.iterrows():\n",
    "    prod_dict[row.article_id] = row.doc\n",
    "\n",
    "customers_dict = {}\n",
    "for i, row in purchase_history.iterrows():\n",
    "    customers_dict[row.customer_id] = \" \".join(map(lambda x: prod_dict[x], row.article_ids))\n",
    "\n",
    "prod_doc_df = pd.DataFrame({\"id\":prod_dict.keys(), \"doc\": prod_dict.values(), \"type\":\"product\"})\n",
    "customer_doc_df = pd.DataFrame({\"id\":customers_dict.keys(), \"doc\": customers_dict.values(), \"type\":\"customer\"})\n",
    "doc_df = pd.concat([prod_doc_df, customer_doc_df])\n",
    "doc_df.to_csv(\"data/corpus.csv\", index=False)\n",
    "\n",
    "del prod_dict, customers_dict, prod_doc_df, customer_doc_df, articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_sell_counts = train.groupby(\"article_id\").size().reset_index().rename(columns={0:\"sell_score\"})\n",
    "train = train.merge(prod_sell_counts, on=\"article_id\",how=\"inner\")\n",
    "train.sell_score = (train.sell_score - train.sell_score.min()) / (train.sell_score.max() - train.sell_score.min())\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"data/corpus.csv\")\n",
    "indexes = corpus[corpus.type == \"product\"].index.tolist()\n",
    "ids = corpus[corpus.type == \"product\"].id.tolist()\n",
    "id_dict = dict(zip(ids, indexes))\n",
    "with open('model_data/prod_id_dict.json', 'w') as fp:\n",
    "    json.dump(id_dict, fp)\n",
    "indexes = corpus[corpus.type == \"customer\"].index.tolist()\n",
    "ids = corpus[corpus.type == \"customer\"].id.tolist()\n",
    "id_dict = dict(zip(ids, indexes))\n",
    "with open('model_data/customer_id_dict.json', 'w') as fp:\n",
    "    json.dump(id_dict, fp)\n",
    "del corpus, indexes, ids, id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec\n",
    "corpus = pd.read_csv('data/corpus.csv', index_col=False)[\"doc\"].apply(lambda x: x.replace('.','')).to_list()\n",
    "Documents = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(corpus)]\n",
    "epochs = 10\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "model= Doc2Vec(dm=0,\n",
    "               vector_size=256,\n",
    "               negative=5,\n",
    "               hs=0,\n",
    "               min_count=2,\n",
    "               sample = 0,\n",
    "               workers=cores)\n",
    "\n",
    "model.build_vocab(Documents)\n",
    "for epoch in range(epochs):\n",
    "    print('iteration {0}'.format(epoch),end = \"\\r\")\n",
    "    model.train(Documents,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=1)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"model_data/d2v.model\")\n",
    "print(\"Model Saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content based Doc2Vec results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"model_data/d2v.model\")\n",
    "model_vectors = model.dv.get_normed_vectors()\n",
    "with open(\"model_data/prod_id_dict.json\",\"r\") as f:\n",
    "    prod_ids = list(json.load(f).values())\n",
    "    prod_d2v_gpu = tf.convert_to_tensor(model_vectors[prod_ids])\n",
    "    customer_d2v_gpu = tf.convert_to_tensor(np.delete(model_vectors, prod_ids,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id_start = 105542\n",
    "step = 0\n",
    "df_list = []\n",
    "for i, customer_vec in enumerate(customer_d2v_gpu):\n",
    "    values, indices  = tf.math.top_k(tf.losses.cosine_similarity(customer_d2v_gpu[0], prod_d2v_gpu), k=150)\n",
    "    df_list.append(pd.DataFrame({\"customer_id\": i + customer_id_start,\n",
    "                                \"prod_ids\":indices,\n",
    "                                \"similarities\":values}))\n",
    "    if i % 25000 == 0 and i != 0:\n",
    "        df = pd.concat(df_list)\n",
    "        df.to_csv(f\"model_data/d2v_sims/{step}.csv\", index=False)\n",
    "        step += 1\n",
    "        df_list = []\n",
    "\n",
    "    print('\\r' + f'{i}: %{round(100*i/customer_d2v_gpu.shape[0], 2)}', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content based TfIdf results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf word level implementation //its run about 11 minutes\n",
    "corpus = pd.read_csv('data/corpus.csv', index_col=False)[\"doc\"].apply(lambda x: x.replace('.','')).to_list()\n",
    "vectorizer = TfidfVectorizer(lowercase=True,max_features=8192, dtype=np.float32)\n",
    "model_vectors = vectorizer.fit_transform(corpus)\n",
    "\n",
    "with open(\"model_data/prod_id_dict.json\",\"r\") as f:\n",
    "    prod_ids = list(json.load(f).values())\n",
    "    prod_tf_gpu = tf.convert_to_tensor(model_vectors[prod_ids].toarray())\n",
    "    del prod_ids\n",
    "\n",
    "with open(\"model_data/customer_id_dict.json\",\"r\") as f:\n",
    "    customer_ids = list(json.load(f).values())\n",
    "    customer_tf = model_vectors[customer_ids].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id_start = 105542\n",
    "step = 0\n",
    "df_list = []\n",
    "for i, customer_vec in enumerate(customer_d2v_gpu):\n",
    "    customer_vec = tf.convert_to_tensor(customer_vec)\n",
    "    values, indices  = tf.math.top_k(tf.losses.cosine_similarity(customer_d2v_gpu[0], prod_d2v_gpu), k=150)\n",
    "    df_list.append(pd.DataFrame({\"customer_id\": i + customer_id_start,\n",
    "                                \"prod_ids\":indices,\n",
    "                                \"similarities\":values}))\n",
    "    if i % 25000 == 0 and i != 0:\n",
    "        df = pd.concat(df_list)\n",
    "        df.to_csv(f\"model_data/tf_idf_sims/{step}.csv\", index=False)\n",
    "        step += 1\n",
    "        df_list = []\n",
    "\n",
    "    print('\\r' + f'{i}: %{round(100*i/customer_d2v_gpu.shape[0], 2)}', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = dd.read_csv('data/transactions_train.csv')[[\"customer_id\",\"article_id\"]]\n",
    "corpus = pd.read_csv(\"data/corpus.csv\")\n",
    "customer_ids = corpus[corpus.type == \"customer\"].id.tolist()\n",
    "del corpus\n",
    "with open(\"model_data/prod_id_dict.json\", \"r\") as f:\n",
    "    prod_dict = json.load(f)\n",
    "\n",
    "number_of_products = len(prod_dict.keys())\n",
    "transactions.article_id = transactions.article_id.apply(lambda x: prod_dict[str(x)], meta=(\"article_id\",\"str\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating customer last 64 order history\n",
    "def get_purchase_history(x):\n",
    "    return list(set(x.article_id))\n",
    "\n",
    "purchase_matrix = transactions.groupby(\"customer_id\").apply(get_purchase_history,meta=(\"history\",\"object\"))\n",
    "purchase_matrix_df = purchase_matrix.compute()\n",
    "del prod_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [np.zeros(50), np.zeros(50), np.zeros(50)]  # each arrX is a numpy array\n",
    "f = h5py.File('model_data/correleation_data.h5', 'w', libver='latest')\n",
    "step = 30000\n",
    "number_of_customer = purchase_matrix_df.size\n",
    "dset = f.create_dataset(\"matrix\", (number_of_customer, number_of_products), dtype=np.int16, compression='gzip')\n",
    "\n",
    "customer_data = []\n",
    "for i, ids in enumerate(purchase_matrix_df):\n",
    "    if i%step == 0 and i !=0:\n",
    "        customer_data = np.asarray(customer_data, dtype=np.int16)\n",
    "        dset[i - customer_data.shape[0]:i] = customer_data\n",
    "        customer_data = []\n",
    "\n",
    "    temp = np.zeros(number_of_products, dtype=np.int16)\n",
    "    temp[ids] = 1\n",
    "    customer_data.append(temp)\n",
    "    print('\\r' + f'{i}: %{round(100*i/number_of_customer, 2)}', end='')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad61a90f8b8ec2120f8b8d3efc4267caee6017c89f89db236bf61b71644f2c7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
