{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import dask\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import dask.array as da\n",
    "import tensorflow as tf\n",
    "from dask import dataframe as dd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from dask_ml.wrappers import Incremental\n",
    "from dask_ml.decomposition import TruncatedSVD\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of data:  31788324\n"
     ]
    }
   ],
   "source": [
    "articles = dd.read_csv('data/articles.csv')\n",
    "transactions = dd.read_csv('data/transactions_train.csv')\n",
    "customers = dd.read_csv('data/customers.csv')\n",
    "customer_purchase_number = transactions.groupby(\"customer_id\").size().to_frame(\"prod_number\").reset_index()\n",
    "transactions = transactions.merge(customer_purchase_number, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "train, test = transactions.random_split([0.9, 0.1], random_state=43)\n",
    "df = test.merge(train[[\"customer_id\"]], on=[\"customer_id\"], how=\"outer\", indicator=True)\n",
    "train = dd.concat([train, df[(df._merge == 'left_only') |  (df.prod_number == 1)][[\"customer_id\", \"article_id\"]]], axis=0, ignore_index=True, interleave_partitions=True, ignore_order=True)\n",
    "df = df[(df._merge == 'both') &( df.prod_number > 1)][[\"customer_id\"]].drop_duplicates()\n",
    "test = test.merge(df, how=\"inner\", on=\"customer_id\")\n",
    "map_test = test.groupby([\"customer_id\"])['article_id'].apply(lambda x: list(x), meta=(\"article_ids\",object)).reset_index().compute().drop_duplicates(subset=[\"customer_id\"])\n",
    "print(\"Len of data: \", len(transactions))\n",
    "del transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling NONE values\n",
    "customers.FN = customers.FN.fillna(0)\n",
    "customers.Active = customers.Active.fillna(0)\n",
    "customers.age = customers.age.fillna(customers.age.mean())\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.fillna(\"not_regular\")\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.apply(lambda x: \"not_regular\" if x == \"NONE\" or x == \"None\" else x, meta=('fashion_news_frequency', 'object'))\n",
    "train.price = train.price.fillna(train.price.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>FN</th>\n",
       "      <th>Active</th>\n",
       "      <th>club_member_status</th>\n",
       "      <th>fashion_news_frequency</th>\n",
       "      <th>age</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>bought_</th>\n",
       "      <th>pricemean</th>\n",
       "      <th>pricestd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>not_regular</td>\n",
       "      <td>49.0</td>\n",
       "      <td>52043ee2162cf5aa7ee79974281641c6f11a68d276429a...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.029419</td>\n",
       "      <td>0.015511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>not_regular</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2973abc54daa8a5f8ccfe9362140c63247c5eee03f1d93...</td>\n",
       "      <td>75</td>\n",
       "      <td>0.030559</td>\n",
       "      <td>0.017683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>not_regular</td>\n",
       "      <td>24.0</td>\n",
       "      <td>64f17e6a330a85798e4998f62d0930d14db8db1c054af6...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.037907</td>\n",
       "      <td>0.016149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>not_regular</td>\n",
       "      <td>54.0</td>\n",
       "      <td>5d36574f52495e81f019b680c843c443bd343d5ca5b1c2...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>Regularly</td>\n",
       "      <td>52.0</td>\n",
       "      <td>25fa5ddee9aac01b35208d01736e57942317d756b32ddd...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.036130</td>\n",
       "      <td>0.012638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id   FN  Active  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...  0.0     0.0   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...  0.0     0.0   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0.0     0.0   \n",
       "3  00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...  0.0     0.0   \n",
       "4  00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...  1.0     1.0   \n",
       "\n",
       "  club_member_status fashion_news_frequency   age  \\\n",
       "0             ACTIVE            not_regular  49.0   \n",
       "1             ACTIVE            not_regular  25.0   \n",
       "2             ACTIVE            not_regular  24.0   \n",
       "3             ACTIVE            not_regular  54.0   \n",
       "4             ACTIVE              Regularly  52.0   \n",
       "\n",
       "                                         postal_code  bought_  pricemean  \\\n",
       "0  52043ee2162cf5aa7ee79974281641c6f11a68d276429a...       18   0.029419   \n",
       "1  2973abc54daa8a5f8ccfe9362140c63247c5eee03f1d93...       75   0.030559   \n",
       "2  64f17e6a330a85798e4998f62d0930d14db8db1c054af6...       16   0.037907   \n",
       "3  5d36574f52495e81f019b680c843c443bd343d5ca5b1c2...        2   0.030492   \n",
       "4  25fa5ddee9aac01b35208d01736e57942317d756b32ddd...       13   0.036130   \n",
       "\n",
       "   pricestd  \n",
       "0  0.015511  \n",
       "1  0.017683  \n",
       "2  0.016149  \n",
       "3  0.000000  \n",
       "4  0.012638  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic statical information extraction\n",
    "prod_count = train.groupby(\"customer_id\").agg({\"customer_id\":\"count\"}).rename(columns={\"customer_id\":\"bought_\"}).reset_index()\n",
    "customers = customers.merge(prod_count, on=\"customer_id\", how=\"inner\")\n",
    "prod_price = train.groupby(\"customer_id\").agg({\"price\":[\"mean\", \"std\"]}).rename(columns={\"customer_id\":\"price_\"}).reset_index()\n",
    "prod_price.columns = list(map(''.join, prod_price.columns.values))\n",
    "customers = customers.merge(prod_price, on=\"customer_id\", how=\"inner\")\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating customer last 64 order history\n",
    "def get_purchase_history(x):\n",
    "    purchase_history = zip(x[\"t_dat\"], x[\"article_id\"])\n",
    "    purchase_history_in_order = sorted(purchase_history, key=lambda i: i[0],reverse=True)[:64]\n",
    "    return [i[1] for i in purchase_history_in_order]\n",
    "\n",
    "\n",
    "purchase_history = train.groupby([\"customer_id\"]).apply(get_purchase_history, meta=(\"article_ids\",object)).reset_index().compute().drop_duplicates(\"customer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting articles to document\n",
    "articles = dd.read_csv('data/articles.csv')\n",
    "def article2doc(x):\n",
    "    def clean_doc(text):\n",
    "        unwanted_chars = ['1','2','3','4','5','6','7','8','9','(',')','[',']']\n",
    "        for chr in unwanted_chars:\n",
    "            text = text.replace(chr, '')\n",
    "        return text\n",
    "\n",
    "    doc =  '. '.join([x.prod_name, x.product_type_name, x.product_group_name, x.graphical_appearance_name, x.colour_group_name,\\\n",
    "                      x.perceived_colour_value_name, x.perceived_colour_master_name, x.department_name, x.index_name, x.index_group_name,\\\n",
    "                      x.section_name, x.garment_group_name, str(x.detail_desc)])[:-1]\n",
    "    return(clean_doc(doc))\n",
    "\n",
    "articles[\"doc\"] = articles.apply(article2doc, axis=1, meta=(\"doc\",\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting customer purchase histroy and product features to document\n",
    "prod_dict = {}\n",
    "for i,row in articles.iterrows():\n",
    "    prod_dict[row.article_id] = row.doc\n",
    "\n",
    "customers_dict = {}\n",
    "for i, row in purchase_history.iterrows():\n",
    "    customers_dict[row.customer_id] = \" \".join(map(lambda x: prod_dict[x], row.article_ids))\n",
    "\n",
    "prod_doc_df = pd.DataFrame({\"id\":prod_dict.keys(), \"doc\": prod_dict.values(), \"type\":\"product\"})\n",
    "customer_doc_df = pd.DataFrame({\"id\":customers_dict.keys(), \"doc\": customers_dict.values(), \"type\":\"customer\"})\n",
    "doc_df = pd.concat([prod_doc_df, customer_doc_df])\n",
    "doc_df.to_csv(\"data/corpus.csv\", index=False)\n",
    "\n",
    "del prod_dict, customers_dict, prod_doc_df, customer_doc_df, articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_sell_counts = train.groupby(\"article_id\").size().reset_index().rename(columns={0:\"sell_score\"})\n",
    "train = train.merge(prod_sell_counts, on=\"article_id\",how=\"inner\")\n",
    "train.sell_score = (train.sell_score - train.sell_score.min()) / (train.sell_score.max() - train.sell_score.min())\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"data/corpus.csv\")\n",
    "indexes = corpus[corpus.type == \"product\"].index.tolist()\n",
    "ids = corpus[corpus.type == \"product\"].id.tolist()\n",
    "id_dict = dict(zip(ids, indexes))\n",
    "with open('model_data/prod_id_dict.json', 'w') as fp:\n",
    "    json.dump(id_dict, fp)\n",
    "indexes = corpus[corpus.type == \"customer\"].index.tolist()\n",
    "ids = corpus[corpus.type == \"customer\"].id.tolist()\n",
    "id_dict = dict(zip(ids, indexes))\n",
    "with open('model_data/customer_id_dict.json', 'w') as fp:\n",
    "    json.dump(id_dict, fp)\n",
    "del corpus, indexes, ids, id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('data/corpus.csv', index_col=False)[\"doc\"].apply(lambda x: x.replace('.','')).to_list()\n",
    "Documents = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s9/93b6jyx57x33y6tt142gsgjc0000gn/T/ipykernel_34989/4144358533.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                workers=cores)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration {0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \"\"\"\n\u001b[0;32m--> 866\u001b[0;31m         total_words, corpus_count = self.scan_vocab(\n\u001b[0m\u001b[1;32m    867\u001b[0m             \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m             \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0mcorpus_iterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTaggedLineDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, corpus_iterable, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# doc2vec\n",
    "epochs = 20\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "model= Doc2Vec(dm=0,\n",
    "               vector_size=256,\n",
    "               negative=5,\n",
    "               hs=0,\n",
    "               min_count=2,\n",
    "               sample = 0,\n",
    "               workers=cores)\n",
    "\n",
    "model.build_vocab(Documents)\n",
    "for epoch in range(epochs):\n",
    "    print('iteration {0}'.format(epoch),end = \"\\r\")\n",
    "    model.train(Documents,\n",
    "                6060,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=1)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"model_data/d2v.model\")\n",
    "print(\"Model Saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content based Doc2Vec results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"model_data/d2v.model\")\n",
    "model_vectors = model.dv.get_normed_vectors()\n",
    "with open(\"model_data/prod_id_dict.json\",\"r\") as f:\n",
    "    prod_ids = list(json.load(f).values())\n",
    "    prod_d2v_gpu = tf.convert_to_tensor(model_vectors[prod_ids])\n",
    "    customer_d2v_gpu = tf.convert_to_tensor(np.delete(model_vectors, prod_ids,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id_start = 105542\n",
    "step = 0\n",
    "df_list = []\n",
    "for i, customer_vec in enumerate(customer_d2v_gpu):\n",
    "    values, indices  = tf.math.top_k(tf.losses.cosine_similarity(customer_d2v_gpu[0], prod_d2v_gpu), k=150)\n",
    "    df_list.append(pd.DataFrame({\"customer_id\": i + customer_id_start,\n",
    "                                \"prod_ids\":indices,\n",
    "                                \"similarities\":values}))\n",
    "    if i % 25000 == 0 and i != 0:\n",
    "        df = pd.concat(df_list)\n",
    "        df.to_csv(f\"model_data/d2v_sims/{step}.csv\", index=False)\n",
    "        step += 1\n",
    "        df_list = []\n",
    "\n",
    "    print('\\r' + f'{i}: %{round(100*i/customer_d2v_gpu.shape[0], 2)}', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content based TfIdf results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\workspace\\H-M\\content_based.ipynb Cell 18'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/content_based.ipynb#ch0000017?line=9'>10</a>\u001b[0m     \u001b[39mdel\u001b[39;00m prod_ids\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/content_based.ipynb#ch0000017?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmodel_data/customer_id_dict.json\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/workspace/H-M/content_based.ipynb#ch0000017?line=11'>12</a>\u001b[0m     customer_ids \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(json\u001b[39m.\u001b[39;49mload(f)\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/content_based.ipynb#ch0000017?line=12'>13</a>\u001b[0m     customer_tf \u001b[39m=\u001b[39m model_vectors[customer_ids]\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/content_based.ipynb#ch0000017?line=13'>14</a>\u001b[0m     \u001b[39mdel\u001b[39;00m customer_ids\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=273'>274</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=274'>275</a>\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=275'>276</a>\u001b[0m     \u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=276'>277</a>\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=277'>278</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=290'>291</a>\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=291'>292</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=292'>293</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=293'>294</a>\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m, object_hook\u001b[39m=\u001b[39;49mobject_hook,\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=294'>295</a>\u001b[0m         parse_float\u001b[39m=\u001b[39;49mparse_float, parse_int\u001b[39m=\u001b[39;49mparse_int,\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=295'>296</a>\u001b[0m         parse_constant\u001b[39m=\u001b[39;49mparse_constant, object_pairs_hook\u001b[39m=\u001b[39;49mobject_pairs_hook, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\json\\__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=351'>352</a>\u001b[0m     \u001b[39mdel\u001b[39;00m kw[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=354'>355</a>\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=355'>356</a>\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=356'>357</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=357'>358</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/__init__.py?line=358'>359</a>\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=331'>332</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=332'>333</a>\u001b[0m     \u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=333'>334</a>\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=334'>335</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=335'>336</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=336'>337</a>\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=337'>338</a>\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=338'>339</a>\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=343'>344</a>\u001b[0m \u001b[39m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=344'>345</a>\u001b[0m \u001b[39ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=345'>346</a>\u001b[0m \u001b[39mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=349'>350</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=350'>351</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=351'>352</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=352'>353</a>\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=353'>354</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/json/decoder.py?line=354'>355</a>\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tf-idf word level implementation //its run about 11 minutes\n",
    "corpus = pd.read_csv('data/corpus.csv', index_col=False)[\"doc\"].apply(lambda x: x.replace('.','')).to_list()\n",
    "vectorizer = TfidfVectorizer(lowercase=True,max_features=8192, dtype=np.float32)\n",
    "model_vectors = vectorizer.fit_transform(corpus)\n",
    "\n",
    "del corpus\n",
    "with open(\"model_data/prod_id_dict.json\",\"r\") as f:\n",
    "    prod_ids = list(json.load(f).values())\n",
    "    prod_tf_gpu = tf.convert_to_tensor(model_vectors[prod_ids].toarray())\n",
    "    del prod_ids\n",
    "with open(\"model_data/customer_id_dict.json\",\"r\") as f:\n",
    "    customer_ids = list(json.load(f).values())\n",
    "    customer_tf = model_vectors[customer_ids].toarray()\n",
    "    del customer_ids, model_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id_start = 105542\n",
    "step = 0\n",
    "df_list = []\n",
    "for i, customer_vec in enumerate(customer_d2v_gpu):\n",
    "    customer_vec = tf.convert_to_tensor(customer_vec)\n",
    "    values, indices  = tf.math.top_k(tf.losses.cosine_similarity(customer_d2v_gpu[0], prod_d2v_gpu), k=150)\n",
    "    df_list.append(pd.DataFrame({\"customer_id\": i + customer_id_start,\n",
    "                                \"prod_ids\":indices,\n",
    "                                \"similarities\":values}))\n",
    "    if i % 25000 == 0 and i != 0:\n",
    "        df = pd.concat(df_list)\n",
    "        df.to_csv(f\"model_data/tf_idf_sims/{step}.csv\", index=False)\n",
    "        step += 1\n",
    "        df_list = []\n",
    "\n",
    "    print('\\r' + f'{i}: %{round(100*i/customer_d2v_gpu.shape[0], 2)}', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = dd.read_csv('data/transactions_train.csv')[[\"customer_id\",\"article_id\"]]\n",
    "corpus = pd.read_csv(\"data/corpus.csv\")\n",
    "customer_ids = corpus[corpus.type == \"customer\"].id.tolist()\n",
    "del corpus\n",
    "with open(\"model_data/prod_id_dict.json\", \"r\") as f:\n",
    "    prod_dict = json.load(f)\n",
    "\n",
    "number_of_products = len(prod_dict.keys())\n",
    "transactions.article_id = transactions.article_id.apply(lambda x: prod_dict[str(x)], meta=(\"article_id\",\"str\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating customer last 64 order history\n",
    "def get_purchase_history(x):\n",
    "    return list(set(x.article_id))\n",
    "\n",
    "purchase_matrix = transactions.groupby(\"customer_id\").apply(get_purchase_history,meta=(\"history\",\"object\"))\n",
    "purchase_matrix_df = purchase_matrix.compute()\n",
    "del prod_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [np.zeros(50), np.zeros(50), np.zeros(50)]  # each arrX is a numpy array\n",
    "f = h5py.File('model_data/correleation_data.h5', 'w', libver='latest')\n",
    "step = 30000\n",
    "number_of_customer = purchase_matrix_df.size\n",
    "dset = f.create_dataset(\"matrix\", (number_of_customer, number_of_products), dtype=np.int16, compression='gzip')\n",
    "\n",
    "customer_data = []\n",
    "for i, ids in enumerate(purchase_matrix_df):\n",
    "    if i%step == 0 and i !=0:\n",
    "        customer_data = np.asarray(customer_data, dtype=np.int16)\n",
    "        dset[i - customer_data.shape[0]:i] = customer_data\n",
    "        customer_data = []\n",
    "\n",
    "    temp = np.zeros(number_of_products, dtype=np.int16)\n",
    "    temp[ids] = 1\n",
    "    customer_data.append(temp)\n",
    "    print('\\r' + f'{i}: %{round(100*i/number_of_customer, 2)}', end='')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad61a90f8b8ec2120f8b8d3efc4267caee6017c89f89db236bf61b71644f2c7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
