{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from dask import dataframe as dd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = dd.read_csv('data/transactions_train.csv', dtype={'article_id': str, 'customer_id': str})\n",
    "if train:\n",
    "    transactions = transactions[(transactions.t_dat >= '2020-03-17') & (transactions.t_dat <= '2020-09-15')]\n",
    "    path = 'data/ensemble_train/'\n",
    "else:\n",
    "    path = 'data/ensemble/'\n",
    "    transactions = transactions[transactions.t_dat >= '2020-03-24']\n",
    "transactions.t_dat = dd.to_datetime(transactions.t_dat) - timedelta(2)\n",
    "transactions[\"week\"] = transactions.t_dat.dt.isocalendar().week\n",
    "transactions = transactions.compute()\n",
    "transactions[\"rebuy_count\"] = transactions.groupby([\"customer_id\", \"article_id\"]).cumcount().astype(int)\n",
    "transactions[\"rebuy_count\"] = transactions.rebuy_count.apply(lambda x: x -1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_solds = transactions[transactions.week > transactions.week.max()-3].groupby([\"article_id\"]).agg({\"article_id\":\"count\"})\\\n",
    "                                   .rename(columns={\"article_id\":\"_count\"}).reset_index()\\\n",
    "                                   .sort_values('_count', ascending=False)\n",
    "most_solds = most_solds.head(10000)\n",
    "transactions = transactions[transactions.article_id.isin(most_solds.article_id)]\n",
    "last_week_articles = transactions[transactions.week == transactions.week.max()].article_id.unique()\n",
    "articles = pd.read_csv(\"data/articles.csv\", dtype={'article_id': str})\n",
    "articles.drop_duplicates(subset=['article_id'], inplace=True)\n",
    "articles = articles[(articles.article_id.isin(most_solds.article_id)) & (articles.article_id.isin(last_week_articles))]\n",
    "del most_solds, last_week_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_customers = transactions.customer_id.unique()\n",
    "customers = pd.read_csv(\"data/customers.csv\", dtype={'customer_id': str})\n",
    "customers.drop_duplicates(subset=['customer_id'], inplace=True)\n",
    "customers = customers[customers.customer_id.isin(active_customers)]\n",
    "del active_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_classification(age):\n",
    "    if age < 19:\n",
    "        return 0\n",
    "    elif age < 29:\n",
    "        return 1\n",
    "    elif age < 49:\n",
    "        return 2\n",
    "    elif age < 59:\n",
    "        return 3\n",
    "    elif age < 69:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "customers[\"age\"] = customers.age.fillna(np.mean(customers.age))\n",
    "customers[\"age_bin\"] = customers.age.map(gender_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_classification(section_name):\n",
    "    if \"womens\" in section_name or \"girl\" in section_name or \"ladies\" in section_name:\n",
    "        return \"woman\"\n",
    "    elif \"men\" in section_name or \"boy\" in section_name or \"boys\" in section_name:\n",
    "        return \"man\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "articles.section_name = articles.section_name.map(lambda x: x.lower())\n",
    "articles[\"gender_group\"] = articles.section_name.apply(gender_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = transactions.merge(articles[[\"article_id\", \"gender_group\"]], on=\"article_id\", how=\"inner\")\n",
    "transactions = transactions.merge(customers[[\"customer_id\",\"age_bin\"]], on=\"customer_id\", how=\"inner\")\n",
    "customer_hist = transactions.groupby(by=\"customer_id\").agg({\"article_id\": lambda x: list(x.values), \"week\": lambda x: list(x.values), \"gender_group\": lambda x : x.mode().iloc[0], \"rebuy_count\": \"mean\"}).reset_index()\n",
    "customers = customers.merge(customer_hist, on=\"customer_id\", how=\"left\")\n",
    "customers.article_id = customers.article_id.fillna(\"\").apply(list)\n",
    "customers.gender_group = customers.gender_group.fillna(\"other\")\n",
    "transactions.price = transactions.price.fillna(transactions.price.mean())\n",
    "prod_price = transactions.groupby(\"customer_id\").agg({\"price\":\"mean\"}).rename(columns={\"customer_id\":\"price_\"}).reset_index()\n",
    "prod_price.columns = list(map(''.join, prod_price.columns.values))\n",
    "customers = customers.merge(prod_price, on=\"customer_id\", how=\"inner\")\n",
    "del customer_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers[\"history\"] = customers.apply(lambda x: sorted(zip(x.week, x.article_id)), axis=1)\n",
    "customers.FN = customers.FN.fillna(0)\n",
    "customers.Active = customers.Active.fillna(0)\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.fillna(\"not_regular\")\n",
    "customers.club_member_status = customers.club_member_status.fillna(\"no-info\")\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.apply(lambda x: \"not_regular\" if x == \"NONE\" or x == \"None\" else x)\n",
    "customers[\"numberOfArticles\"] = customers.apply(lambda x: len(x.history), axis=1)\n",
    "customers = customers.drop(columns=[\"postal_code\", \"week\", \"article_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_fashion_news(name):\n",
    "    return 1 if name == 'not_regular' else 0\n",
    "def map_club_member(name):\n",
    "    return 1 if name == 'ACTIVE' else 0\n",
    "\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.map(map_fashion_news)\n",
    "customers.club_member_status = customers.club_member_status.map(map_club_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article2doc(x):\n",
    "    def clean_doc(text):\n",
    "        unwanted_chars = ['1','2','3','4','5','6','7','8','9','(',')','[',']']\n",
    "        for chr in unwanted_chars:\n",
    "            text = text.replace(chr, '')\n",
    "        return text\n",
    "\n",
    "    doc =  '. '.join([x.prod_name, x.product_type_name, x.product_group_name, x.graphical_appearance_name, x.colour_group_name,\\\n",
    "                      x.perceived_colour_value_name, x.perceived_colour_master_name, x.department_name, x.index_name, x.index_group_name,\\\n",
    "                      x.section_name, x.garment_group_name, str(x.detail_desc)])[:-1]\n",
    "    return(clean_doc(doc.lower()))\n",
    "\n",
    "articles[\"doc\"] = articles.apply(article2doc, axis=1)\n",
    "article_info = transactions.groupby([\"article_id\"]).agg({\"price\":\"mean\", \"rebuy_count\":\"mean\", \"age_bin\": lambda x : x.mode().iloc[0]}).reset_index()\n",
    "articles = articles.merge(article_info, on=\"article_id\", how=\"inner\")\n",
    "most_solds = transactions[transactions.week > transactions.week.max()-3].groupby([\"article_id\"]).agg({\"customer_id\":\"count\"})\\\n",
    "                                      .rename(columns={\"customer_id\":\"prod_sold_count\"}).reset_index()\n",
    "articles = articles.merge(most_solds, on=\"article_id\", how=\"inner\")\n",
    "articles = articles[[\"article_id\",\"doc\",\"gender_group\", \"price\", \"rebuy_count\", \"age_bin\", \"prod_sold_count\"]]\n",
    "weekly_sales = transactions.groupby([\"article_id\", \"week\"]).agg({\"customer_id\":\"count\"}).reset_index()\n",
    "last_week_sales = weekly_sales[weekly_sales.week == weekly_sales.week.max()]\n",
    "weekly_sales = weekly_sales.merge(last_week_sales[[\"article_id\",\"customer_id\"]], on=[\"article_id\"], how=\"inner\")\n",
    "weekly_sales[\"quotient\"] = weekly_sales.customer_id_y / weekly_sales.customer_id_x\n",
    "articles = articles.merge(weekly_sales[[\"article_id\",\"quotient\"]], on=\"article_id\", how=\"inner\")\n",
    "\n",
    "del article_info, most_solds, weekly_sales, last_week_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers[\"doc\"] = customers.history.apply(lambda x: list(set([i[1] for i in x])))\n",
    "temp_dict = {}\n",
    "for i,row in articles.iterrows():\n",
    "    temp_dict[row.article_id] = row.doc\n",
    "\n",
    "customers[\"doc\"] = customers.doc.apply(lambda x:  \". \".join([temp_dict[i] for i in x]))\n",
    "del temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Based Corpus Data Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df1 = articles[[\"article_id\",\"doc\"]].rename(columns={\"article_id\":\"doc_id\"}).copy()\n",
    "articles.drop(columns=[\"doc\"], inplace=True)\n",
    "doc_df2 = customers[[\"customer_id\",\"doc\"]].rename(columns={\"customer_id\":\"doc_id\"}).copy()\n",
    "customers.drop(columns=[\"doc\"], inplace=True)\n",
    "doc_df1[\"type\"] = \"product\"\n",
    "doc_df2[\"type\"] = \"customer\"\n",
    "doc_df = doc_df1.append(doc_df2)\n",
    "doc_df.to_csv(path+\"corpus.csv\", index=False)\n",
    "\n",
    "del doc_df, doc_df1, doc_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collabrative Data Creating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last_week = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions[[\"customer_id\",\"article_id\",\"week\"]].to_pickle(\"data/transactions.pkl\")\n",
    "last_week = transactions.week.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.FN = customers.FN.astype('category').cat.codes\n",
    "customers.Active = customers.Active.astype('category').cat.codes\n",
    "customers.club_member_status = customers.club_member_status.astype('category').cat.codes\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.astype('category').cat.codes\n",
    "customers.age_bin = customers.age_bin.astype('category').cat.codes\n",
    "customers.gender_group = customers.gender_group.astype('category').cat.codes\n",
    "customers.age = (customers.age - customers.age.min()) / (customers.age.max() - customers.age.min())\n",
    "customers.numberOfArticles = (customers.numberOfArticles - customers.numberOfArticles.min()) / (customers.numberOfArticles.max() - customers.numberOfArticles.min())\n",
    "if train != True:\n",
    "    customers.drop_duplicates(subset=[\"customer_id\"], inplace=True)\n",
    "    customers.to_pickle(\"data/customers.pkl\")\n",
    "    print(\"Customers saved for prediction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.gender_group = articles.gender_group.astype('category').cat.codes\n",
    "articles.age_bin = articles.age_bin.astype('category').cat.codes\n",
    "articles.price = (articles.price - articles.price.min()) / (articles.price.max() - articles.price.min())\n",
    "articles.rebuy_count = (articles.rebuy_count - articles.rebuy_count.min()) / (articles.rebuy_count.max() - articles.rebuy_count.min())\n",
    "articles.prod_sold_count = (articles.prod_sold_count - articles.prod_sold_count.min()) / (articles.prod_sold_count.max() - articles.prod_sold_count.min())\n",
    "articles.quotient = (articles.quotient - articles.quotient.min()) / (articles.quotient.max() - articles.quotient.min())\n",
    "if train != True:\n",
    "    articles.drop_duplicates(subset=[\"article_id\"], inplace=True)\n",
    "    articles.to_pickle(\"data/articles.pkl\")\n",
    "    print(\"Articles saved for prediction...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Data Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dd.read_csv('data/transactions_train.csv', dtype={'article_id': str, 'customer_id': str})[[\"customer_id\", \"article_id\", \"t_dat\"]]\n",
    "data = data[data.t_dat >= '2020-09-15'].compute()\n",
    "data[\"label\"] = 1.0\n",
    "data = data.drop_duplicates(subset=[\"customer_id\", \"article_id\"])\n",
    "data.drop(columns=[\"t_dat\"], inplace=True)\n",
    "article_id_list = list(articles.article_id.values)\n",
    "data = data[data.article_id.isin(article_id_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_info = data.groupby([\"customer_id\"]).agg({\"article_id\": lambda x: list(set(x))}).reset_index()\n",
    "group_info[\"prod_num\"] = group_info.article_id.apply(lambda x: len(x))\n",
    "customer_id_list = data.customer_id.to_list()\n",
    "prod_id_list = data.article_id.to_list()\n",
    "label_list = data.label.to_list()\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in group_info.iterrows():\n",
    "    temp_articles = random.sample(article_id_list, 300)\n",
    "    step = 0\n",
    "    for id in temp_articles:\n",
    "        if id not in row.article_id:\n",
    "            customer_id_list.append(row.customer_id)\n",
    "            label_list.append(0.0)\n",
    "            prod_id_list.append(id)\n",
    "            step += 1\n",
    "        if step >= 250:\n",
    "            break\n",
    "    print('\\r' + f'{i}: %{round(100*i/group_info.shape[0], 2)}', end='')\n",
    "\n",
    "data = pd.DataFrame({\"customer_id\": customer_id_list, \"article_id\": prod_id_list, \"label\": label_list, \"week\": last_week+1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dd.from_pandas(data, npartitions=8)\n",
    "data = data.merge(articles.rename(columns={\"age_bin\":\"prod_age_bin\", \"gender_group\":\"prod_gender_group\", \"rebuy_count\":\"prod_rebuy_count\",\"price\":\"prod_avg_price\"}), on=\"article_id\", how=\"inner\")\n",
    "data = data.merge(customers.rename(columns={\"age_bin\":\"customer_age_bin\", \"gender_group\":\"customer_gender_group\", \"rebuy_count\":\"customer_rebuy_count\",\"price\":\"customer_avg_price\"}), on=\"customer_id\", how=\"inner\")\n",
    "data[\"same_prod_rebuy_count\"] = data.apply(lambda x: [i[1] for i in x.history].count(x.article_id), axis=1, meta=(\"same_prod_rebuy_count\",\"int\"))\n",
    "del articles, customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_purchased_time(x):\n",
    "    week_passed = 7*4\n",
    "    if x.same_prod_rebuy_count != 0:\n",
    "        for i in x.history:\n",
    "            if i[1] == x.article_id:\n",
    "                week_passed = i[0]\n",
    "        return x.week - week_passed\n",
    "    else:\n",
    "        return week_passed\n",
    "\n",
    "def get_avg_purchase_time(x):\n",
    "    if x.same_prod_rebuy_count != 0:\n",
    "        week = [x.week]\n",
    "        for i in x.history:\n",
    "            if i[1] == x.article_id:\n",
    "                week.append(i[0])\n",
    "        return (max(week) - min(week)) / (len(week) - 1)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"time_passed_last_purchase\"] = data.apply(lambda x: get_last_purchased_time(x), axis=1,  meta=(\"time_passed_last_purchase\",\"int\"))\n",
    "data[\"avg_purchase_time\"] = data.apply(lambda x: get_avg_purchase_time(x), axis=1,  meta=(\"time_passed_last_purchase\",\"float\"))\n",
    "data.drop(columns=[\"history\", \"week\"])\n",
    "#data.same_prod_rebuy_count = (data.same_prod_rebuy_count - data.same_prod_rebuy_count.min()) / (data.same_prod_rebuy_count.max() - data.same_prod_rebuy_count.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(path + \"ensemble_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Data Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf as gd\n",
    "from dask import dataframe as dd\n",
    "\n",
    "customers = pd.read_pickle(\"data/ensemble/customers.pkl\")\n",
    "customers.drop_duplicates(subset=[\"customer_id\"], inplace=True)\n",
    "articles = pd.read_pickle(\"data/ensemble/articles.pkl\")\n",
    "articles.drop_duplicates(subset=[\"article_id\"], inplace=True)\n",
    "article_ids = articles.article_id.values.tolist()\n",
    "customer_ids = customers.customer_id.values\n",
    "\n",
    "batch_size = 1024\n",
    "article_ids = article_ids * batch_size\n",
    "for batch_i in range(batch_size, len(customer_ids), batch_size):\n",
    "    customer_ids_batch = customer_ids[batch_i-batch_size:batch_i]\n",
    "    customer_ids_batch = np.repeat(customer_ids_batch, len(article_ids)/batch_size)\n",
    "\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame({\"customer_id\": customer_ids_batch, \"article_id\": article_ids, \"week\": last_week+1})\n",
    "    df = df.merge(articles.rename(columns={\"age_bin\":\"prod_age_bin\", \"gender_group\":\"prod_gender_group\", \"rebuy_count\":\"prod_rebuy_count\",\"price\":\"prod_avg_price\"}), on=\"article_id\", how=\"inner\")\n",
    "    df = df.merge(customers.rename(columns={\"age_bin\":\"customer_age_bin\", \"gender_group\":\"customer_gender_group\", \"rebuy_count\":\"customer_rebuy_count\",\"price\":\"customer_avg_price\"}), on=\"customer_id\", how=\"inner\")\n",
    "    df[\"same_prod_rebuy_count\"] = df.apply(lambda x: [i[1] for i in x.history].count(x.article_id), axis=1)\n",
    "    df[\"time_passed_last_purchase\"] = df.apply(lambda x: get_last_purchased_time(x), axis=1)\n",
    "    df[\"avg_purchase_time\"] = df.apply(lambda x: get_avg_purchase_time(x), axis=1)\n",
    "    df.drop(columns=[\"history\", \"week\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad61a90f8b8ec2120f8b8d3efc4267caee6017c89f89db236bf61b71644f2c7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
