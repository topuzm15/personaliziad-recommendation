{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from dask import dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = dd.read_csv('data/customers.csv')\n",
    "transactions = dd.read_csv('data/transactions_train.csv')\n",
    "customer_purchase_number = transactions.groupby(\"customer_id\").size().to_frame(\"prod_number\").reset_index()\n",
    "transactions = transactions.merge(customer_purchase_number, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "train, test = transactions.random_split([0.9, 0.1], random_state=43)\n",
    "df = test.merge(train[[\"customer_id\"]], on=[\"customer_id\"], how=\"outer\", indicator=True)\n",
    "train = dd.concat([train, df[(df._merge == 'left_only') |  (df.prod_number == 1)][[\"customer_id\", \"article_id\"]]], axis=0, ignore_index=True, interleave_partitions=True, ignore_order=True)\n",
    "df = df[(df._merge == 'both') &( df.prod_number > 1)][[\"customer_id\"]].drop_duplicates()\n",
    "test = test.merge(df, how=\"inner\", on=\"customer_id\")\n",
    "print(\"Len of data: \", len(transactions))\n",
    "del customer_purchase_number, transactions, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.FN = customers.FN.fillna(0)\n",
    "customers.Active = customers.Active.fillna(0)\n",
    "customers.age = customers.age.fillna(customers.age.mean())\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.fillna(\"not_regular\")\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.apply(lambda x: \"not_regular\" if x == \"NONE\" or x == \"None\" else x, meta=('fashion_news_frequency', 'object'))\n",
    "prod_count = train.groupby(\"customer_id\").agg({\"customer_id\":\"count\"}).rename(columns={\"customer_id\":\"number_of_product\"}).reset_index()\n",
    "customers = customers.merge(prod_count, on=\"customer_id\", how=\"inner\")\n",
    "prod_price = train.groupby(\"customer_id\").agg({\"price\":[\"mean\", \"std\"]}).rename(columns={\"customer_id\":\"price_\"}).reset_index()\n",
    "prod_price.columns = list(map(''.join, prod_price.columns.values))\n",
    "customers = customers.merge(prod_price, on=\"customer_id\", how=\"inner\")\n",
    "customers = customers.drop(columns=\"postal_code\").compute()\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_fashion_news(name):\n",
    "    return 1 if name == 'not_regular' else 0\n",
    "def map_club_member(name):\n",
    "    return 1 if name == 'ACTIVE' else 0\n",
    "\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.map(map_fashion_news)\n",
    "customers.club_member_status = customers.club_member_status.map(map_club_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806879, 10)\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(transactions):\n",
    "    transactions = transactions.groupby([\"customer_id\"])['article_id']\\\n",
    "                                .apply(lambda x: list(x), meta=(\"article_ids\",object))\\\n",
    "                                .reset_index().compute().drop_duplicates(subset=[\"customer_id\"])\n",
    "\n",
    "    transactions[\"hist_len\"] = transactions.article_ids.apply(lambda x: 12 if len(x)>12 else len(x))\n",
    "    transactions[\"prod_ids\"] = transactions.apply(lambda x: random.sample(x.article_ids, x.hist_len), axis=1)\n",
    "    transactions = transactions[[\"customer_id\", \"prod_ids\"]]\n",
    "    return transactions\n",
    "\n",
    "data = prepare_data(test)\n",
    "data = data.merge(customers, on=\"customer_id\", how=\"inner\")\n",
    "data.to_pickle(\"data/ensemble_data.pkl\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, lr, prod_size = 12):\n",
    "        self.lr = lr\n",
    "        self.prod_size = prod_size\n",
    "\n",
    "    def top_k_score(self, predics, actual):\n",
    "        scores = []\n",
    "        actual = actual[:self.prod_size]\n",
    "        for i,pred in enumerate(predics):\n",
    "            hit = actual.count(pred)\n",
    "            if hit > 0:\n",
    "                scores.append(hit / (i+1.0))\n",
    "        \n",
    "        if len(scores) > 0:\n",
    "            score = np.sum(scores) / len(set(actual))\n",
    "            return score if score < 0 else 1\n",
    "        return 0.00001\n",
    "\n",
    "\n",
    "    def lr_schedular(self, epoch):\n",
    "        if epoch >= 5:\n",
    "            self.optimizer.lr = 0.001\n",
    "\n",
    "    def __call__(self, predict, actual):\n",
    "        return tf.math.log(-self.top_k_score(predict, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 16:43:27.327005: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "customers = data.customer_id\n",
    "train_y = data.prod_ids\n",
    "train_x = tf.data.Dataset.from_tensor_slices(data.drop(columns=[\"customer_id\", \"prod_ids\"]).values).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble(tf.keras.Model):\n",
    "    def __init__(self, input_dim, dropout_rate = 0.1):\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(tf.keras.layers.Dense(units = 50, activation= 'tanh', input_dim = input_dim))\n",
    "        self.model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "        self.model.add(tf.keras.layers.Dense(units = 50, activation= 'tanh'))\n",
    "        self.model.add(tf.keras.layers.BatchNormalization())\n",
    "        self.model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "        self.model.add(tf.keras.layers.Dense(units = 10, activation= 'tanh'))\n",
    "        self.model.add(tf.keras.layers.BatchNormalization())\n",
    "        self.model.add(tf.keras.layers.Dense(units = 3, activation= 'sigmoid'))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6ab6d9cbf7d38f3ac8fdd22c9ef5fa94a9494168d372e8011cb61f1f28f62b9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
