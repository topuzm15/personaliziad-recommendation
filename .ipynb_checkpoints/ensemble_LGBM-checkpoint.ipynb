{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from dask import dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id                    object\n",
       "article_id                      int64\n",
       "label                         float64\n",
       "prod_gender_group            category\n",
       "prod_avg_price                float64\n",
       "prod_rebuy_count              float64\n",
       "prod_age_bin                 category\n",
       "prod_sold_count               float64\n",
       "quotient                      float64\n",
       "customer_index                  int64\n",
       "FN                           category\n",
       "Active                       category\n",
       "club_member_status           category\n",
       "fashion_news_frequency       category\n",
       "age                           float64\n",
       "customer_age_bin             category\n",
       "customer_gender_group        category\n",
       "customer_rebuy_count          float64\n",
       "customer_avg_price            float64\n",
       "numberOfArticles              float64\n",
       "same_prod_rebuy_count         float64\n",
       "time_passed_last_purchase     float64\n",
       "avg_purchase_time             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dd.read_csv(\"data/ensemble_train/ensemble_data.csv/*.part\", blocksize=\"64MB\")\n",
    "data = data.drop([\"Unnamed: 0\"], axis=1)\n",
    "data.time_passed_last_purchase = data.time_passed_last_purchase.astype(np.float64)\n",
    "data.prod_gender_group = data.prod_gender_group.astype('category')\n",
    "data.prod_age_bin = data.prod_age_bin.astype('category')\n",
    "data.FN = data.FN.astype('category')\n",
    "data.Active = data.Active.astype('category')\n",
    "data.club_member_status = data.club_member_status.astype('category')\n",
    "data.fashion_news_frequency = data.fashion_news_frequency.astype('category')\n",
    "data.customer_age_bin = data.customer_age_bin.astype('category')\n",
    "data.customer_gender_group = data.customer_gender_group.astype('category')\n",
    "data.dtypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = data.customer_index.max().compute()\n",
    "train = data[data[\"customer_index\"] <= int(number_of_samples * 0.9)].compute()\n",
    "train.drop(columns=\"customer_index\", inplace=True)\n",
    "\n",
    "test = data[data[\"customer_index\"] > int(number_of_samples * 0.9)].compute()\n",
    "test.drop(columns=\"customer_index\", inplace=True)\n",
    "q_train = train.groupby(\"customer_id\")[\"customer_id\"].count()\n",
    "train_label = train.label\n",
    "train = train.drop([\"label\", \"customer_id\", \"article_id\"], axis=1)\n",
    "\n",
    "q_test = test.groupby(\"customer_id\")[\"customer_id\"].count()\n",
    "test_label = test.label\n",
    "test = test.drop([\"label\", \"customer_id\", \"article_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\topuz\\anaconda3\\envs\\tf\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "C:\\Users\\topuz\\anaconda3\\envs\\tf\\lib\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "C:\\Users\\topuz\\anaconda3\\envs\\tf\\lib\\site-packages\\lightgbm\\basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's ndcg@3: 0.124696\tvalid_0's ndcg@7: 0.145493\tvalid_0's ndcg@12: 0.167735\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[2]\tvalid_0's ndcg@3: 0.123928\tvalid_0's ndcg@7: 0.144043\tvalid_0's ndcg@12: 0.165996\n",
      "[3]\tvalid_0's ndcg@3: 0.13315\tvalid_0's ndcg@7: 0.153663\tvalid_0's ndcg@12: 0.176615\n",
      "[4]\tvalid_0's ndcg@3: 0.135169\tvalid_0's ndcg@7: 0.154629\tvalid_0's ndcg@12: 0.178029\n",
      "[5]\tvalid_0's ndcg@3: 0.138138\tvalid_0's ndcg@7: 0.157366\tvalid_0's ndcg@12: 0.18062\n",
      "[6]\tvalid_0's ndcg@3: 0.139125\tvalid_0's ndcg@7: 0.159193\tvalid_0's ndcg@12: 0.181639\n",
      "[7]\tvalid_0's ndcg@3: 0.139991\tvalid_0's ndcg@7: 0.160573\tvalid_0's ndcg@12: 0.18285\n",
      "[8]\tvalid_0's ndcg@3: 0.140646\tvalid_0's ndcg@7: 0.160256\tvalid_0's ndcg@12: 0.183002\n",
      "[9]\tvalid_0's ndcg@3: 0.140914\tvalid_0's ndcg@7: 0.161181\tvalid_0's ndcg@12: 0.183933\n",
      "[10]\tvalid_0's ndcg@3: 0.141954\tvalid_0's ndcg@7: 0.162355\tvalid_0's ndcg@12: 0.185497\n",
      "[11]\tvalid_0's ndcg@3: 0.141957\tvalid_0's ndcg@7: 0.162318\tvalid_0's ndcg@12: 0.18569\n",
      "[12]\tvalid_0's ndcg@3: 0.142956\tvalid_0's ndcg@7: 0.162429\tvalid_0's ndcg@12: 0.18622\n",
      "[13]\tvalid_0's ndcg@3: 0.142103\tvalid_0's ndcg@7: 0.162119\tvalid_0's ndcg@12: 0.186276\n",
      "[14]\tvalid_0's ndcg@3: 0.142159\tvalid_0's ndcg@7: 0.16154\tvalid_0's ndcg@12: 0.185678\n",
      "[15]\tvalid_0's ndcg@3: 0.14213\tvalid_0's ndcg@7: 0.16135\tvalid_0's ndcg@12: 0.185681\n",
      "[16]\tvalid_0's ndcg@3: 0.14214\tvalid_0's ndcg@7: 0.161488\tvalid_0's ndcg@12: 0.185578\n",
      "[17]\tvalid_0's ndcg@3: 0.141825\tvalid_0's ndcg@7: 0.161772\tvalid_0's ndcg@12: 0.186018\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's ndcg@3: 0.142956\tvalid_0's ndcg@7: 0.162429\tvalid_0's ndcg@12: 0.18622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x229a83ab910>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric= \"ndcg\",\n",
    "    learning_rate=0.00001,\n",
    "    max_depth=512,\n",
    "    num_leaves=4096,\n",
    "    num_iterations=30\n",
    "    \n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train, train_label, group=q_train.values, eval_set=[(test, test_label)], eval_group=[q_test.values], eval_at=[12, 7, 3],\n",
    "    callbacks=[lgb.callback.log_evaluation(), lgb.callback.early_stopping(5, first_metric_only=False)],\n",
    "    categorical_feature = 'auto'\n",
    ")\n",
    "\n",
    "model.booster_.save_model('weights/lbm_lamda_ranker.txt',  num_iteration=model.best_iteration_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import cudf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "model = lgb.Booster(model_file='weights/lbm_lamda_ranker.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_pickle(\"data/ensemble/customers.pkl\")\n",
    "customers.drop(columns=\"customer_index\", inplace=True)\n",
    "customers.drop_duplicates(subset=[\"customer_id\"], inplace=True)\n",
    "customers = customers.sort_values(by=[\"customer_id\"])\n",
    "articles = pd.read_pickle(\"data/ensemble/articles.pkl\")\n",
    "articles.drop_duplicates(subset=[\"article_id\"], inplace=True)\n",
    "customer_hist = pd.read_pickle(\"data/ensemble/customer_hist.pkl\")\n",
    "customer_hist.drop_duplicates(subset=[\"customer_id\", \"article_id\"], inplace=True)\n",
    "article_ids = articles.article_id.values.tolist()\n",
    "customer_ids = customers.customer_id.values\n",
    "customers = cudf.DataFrame.from_pandas(customers)\n",
    "customer_hist = cudf.DataFrame.from_pandas(customer_hist)\n",
    "articles = cudf.DataFrame.from_pandas(articles)\n",
    "batch_size = 512\n",
    "article_ids = article_ids * batch_size\n",
    "article_ids_str = ('0' + articles.article_id.astype(str)).to_numpy()\n",
    "submission = pd.DataFrame({\"customer_id\":[],\"predict\":[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556032: %81.15"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_list = []\n",
    "loop_size = len(customer_ids) + batch_size\n",
    "for batch_i in range(batch_size, loop_size, batch_size):\n",
    "    customer_ids_batch = customer_ids[0:batch_size]\n",
    "    customer_ids_batch = np.repeat(customer_ids_batch, len(article_ids)/batch_size)\n",
    "    df = cudf.DataFrame({\"customer_id\": customer_ids_batch, \"article_id\": article_ids, \"week\": 39})\n",
    "    df = df.merge(articles.rename(columns={\"age_bin\":\"prod_age_bin\", \"gender_group\":\"prod_gender_group\", \\\n",
    "                                           \"rebuy_count\":\"prod_rebuy_count\",\"price\":\"prod_avg_price\"}), on=\"article_id\", how=\"inner\")\n",
    "    df = df.merge(customers.rename(columns={\"age_bin\":\"customer_age_bin\", \"gender_group\":\"customer_gender_group\",\\\n",
    "                                            \"rebuy_count\":\"customer_rebuy_count\",\"price\":\"customer_avg_price\", \\\n",
    "                                            \"article_id\":\"article_hist\", \"week\":\"week_hist\"}), on=\"customer_id\", how=\"inner\")\n",
    "    df = df.merge(customer_hist, on=[\"customer_id\",\"article_id\"], how=\"left\")\n",
    "    df.same_prod_rebuy_count = df.same_prod_rebuy_count.fillna(0)\n",
    "    df.avg_purchase_time = df.avg_purchase_time.fillna(0)\n",
    "    df.time_passed_last_purchase = df.time_passed_last_purchase.fillna(39 - 29) # 6 mounths is nearly 29 week\n",
    "    df.time_passed_last_purchase = df.apply(lambda x: x.week - x.time_passed_last_purchase)\n",
    "    df.drop(columns=[\"customer_id\", \"article_id\",\"week\"], inplace=True)\n",
    "    ensemble_scores = model.predict(df.to_numpy()).reshape((batch_size, articles.shape[0]))\n",
    "    indices = np.flip(np.argsort(ensemble_scores, axis=1), axis=1)[:,-12:]\n",
    "    predicts = list(map(lambda x: ' '.join(x), article_ids_str[indices]))\n",
    "    submission = pd.concat([submission, pd.DataFrame({\"customer_id\":customer_ids[0:batch_size],\"predict\":predicts})])\n",
    "    del df\n",
    "    gc.collect()\n",
    "    print('\\r' + f'{batch_i}: %{round(100*batch_i/loop_size, 2)}', end='')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad61a90f8b8ec2120f8b8d3efc4267caee6017c89f89db236bf61b71644f2c7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
