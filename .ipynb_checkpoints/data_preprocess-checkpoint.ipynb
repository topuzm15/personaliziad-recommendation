{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from dask import dataframe as dd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = dd.read_csv('data/transactions_train.csv', dtype={'article_id': int, 'customer_id': str})\n",
    "if train:\n",
    "    transactions = transactions[(transactions.t_dat >= '2020-03-17') & (transactions.t_dat <= '2020-09-15')]\n",
    "    path = 'data/ensemble_train/'\n",
    "else:\n",
    "    path = 'data/ensemble/'\n",
    "    transactions = transactions[transactions.t_dat >= '2020-03-24']\n",
    "transactions.t_dat = dd.to_datetime(transactions.t_dat) - timedelta(2)\n",
    "transactions[\"week\"] = transactions.t_dat.dt.isocalendar().week\n",
    "transactions.week = transactions.week.astype(int)\n",
    "transactions = transactions.compute()\n",
    "transactions[\"rebuy_count\"] = transactions.groupby([\"customer_id\", \"article_id\"]).cumcount().astype(int)\n",
    "transactions[\"rebuy_count\"] = transactions.rebuy_count.apply(lambda x: x -1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_solds = transactions[transactions.week > transactions.week.max()-3].groupby([\"article_id\"]).agg({\"article_id\":\"count\"})\\\n",
    "                                   .rename(columns={\"article_id\":\"_count\"}).reset_index()\\\n",
    "                                   .sort_values('_count', ascending=False)\n",
    "most_solds = most_solds.head(10000)\n",
    "transactions = transactions[transactions.article_id.isin(most_solds.article_id)]\n",
    "last_week_articles = transactions[transactions.week == transactions.week.max()].article_id.unique()\n",
    "articles = pd.read_csv(\"data/articles.csv\", dtype={'article_id': int})\n",
    "articles.drop_duplicates(subset=['article_id'], inplace=True)\n",
    "articles = articles[(articles.article_id.isin(most_solds.article_id)) & (articles.article_id.isin(last_week_articles))]\n",
    "del most_solds, last_week_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_customers = transactions.customer_id.unique()\n",
    "customers = pd.read_csv(\"data/customers.csv\", dtype={'customer_id': str})\n",
    "customers.drop_duplicates(subset=['customer_id'], inplace=True)\n",
    "customers = customers[customers.customer_id.isin(active_customers)]\n",
    "del active_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_classification(age):\n",
    "    if age < 19:\n",
    "        return 0\n",
    "    elif age < 29:\n",
    "        return 1\n",
    "    elif age < 49:\n",
    "        return 2\n",
    "    elif age < 59:\n",
    "        return 3\n",
    "    elif age < 69:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "customers[\"age\"] = customers.age.fillna(np.mean(customers.age))\n",
    "customers[\"age_bin\"] = customers.age.map(gender_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_classification(section_name):\n",
    "    if \"womens\" in section_name or \"girl\" in section_name or \"ladies\" in section_name:\n",
    "        return \"woman\"\n",
    "    elif \"men\" in section_name or \"boy\" in section_name or \"boys\" in section_name:\n",
    "        return \"man\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "articles.section_name = articles.section_name.map(lambda x: x.lower())\n",
    "articles[\"gender_group\"] = articles.section_name.apply(gender_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = transactions.merge(articles[[\"article_id\", \"gender_group\"]], on=\"article_id\", how=\"inner\")\n",
    "transactions = transactions.merge(customers[[\"customer_id\",\"age_bin\"]], on=\"customer_id\", how=\"inner\")\n",
    "customer_hist = transactions.groupby(by=\"customer_id\").agg({\"article_id\": lambda x: list(x.values), \"week\": lambda x: list(x.values), \"gender_group\": lambda x : x.mode().iloc[0], \"rebuy_count\": \"mean\"}).reset_index()\n",
    "customers = customers.merge(customer_hist, on=\"customer_id\", how=\"left\")\n",
    "customers.article_id = customers.article_id.fillna(\"\").apply(list)\n",
    "customers.gender_group = customers.gender_group.fillna(\"other\")\n",
    "transactions.price = transactions.price.fillna(transactions.price.mean())\n",
    "prod_price = transactions.groupby(\"customer_id\").agg({\"price\":\"mean\"}).rename(columns={\"customer_id\":\"price_\"}).reset_index()\n",
    "prod_price.columns = list(map(''.join, prod_price.columns.values))\n",
    "customers = customers.merge(prod_price, on=\"customer_id\", how=\"inner\")\n",
    "del customer_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_customer_hist(week, article_id):\n",
    "    customer_hist = np.asarray(sorted(zip(week, article_id), reverse=True))\n",
    "    return customer_hist[:,0], customer_hist[:,1]\n",
    "\n",
    "customers[[\"week\", \"article_id\"]] = customers.apply(lambda x: sort_customer_hist(x.week, x.article_id), axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.FN = customers.FN.fillna(0)\n",
    "customers.Active = customers.Active.fillna(0)\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.fillna(\"not_regular\")\n",
    "customers.club_member_status = customers.club_member_status.fillna(\"no-info\")\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.apply(lambda x: \"not_regular\" if x == \"NONE\" or x == \"None\" else x)\n",
    "customers[\"numberOfArticles\"] = customers.apply(lambda x: len(x.article_id), axis=1)\n",
    "customers = customers.drop(columns=[\"postal_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_fashion_news(name):\n",
    "    return 1 if name == 'not_regular' else 0\n",
    "def map_club_member(name):\n",
    "    return 1 if name == 'ACTIVE' else 0\n",
    "\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.map(map_fashion_news)\n",
    "customers.club_member_status = customers.club_member_status.map(map_club_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article2doc(x):\n",
    "    def clean_doc(text):\n",
    "        unwanted_chars = ['1','2','3','4','5','6','7','8','9','(',')','[',']']\n",
    "        for chr in unwanted_chars:\n",
    "            text = text.replace(chr, '')\n",
    "        return text\n",
    "\n",
    "    doc =  '. '.join([x.prod_name, x.product_type_name, x.product_group_name, x.graphical_appearance_name, x.colour_group_name,\\\n",
    "                      x.perceived_colour_value_name, x.perceived_colour_master_name, x.department_name, x.index_name, x.index_group_name,\\\n",
    "                      x.section_name, x.garment_group_name, str(x.detail_desc)])[:-1]\n",
    "    return(clean_doc(doc.lower()))\n",
    "\n",
    "articles[\"doc\"] = articles.apply(article2doc, axis=1)\n",
    "article_info = transactions.groupby([\"article_id\"]).agg({\"price\":\"mean\", \"rebuy_count\":\"mean\", \"age_bin\": lambda x : x.mode().iloc[0]}).reset_index()\n",
    "articles = articles.merge(article_info, on=\"article_id\", how=\"inner\")\n",
    "most_solds = transactions[transactions.week > transactions.week.max()-3].groupby([\"article_id\"]).agg({\"customer_id\":\"count\"})\\\n",
    "                                      .rename(columns={\"customer_id\":\"prod_sold_count\"}).reset_index()\n",
    "articles = articles.merge(most_solds, on=\"article_id\", how=\"inner\")\n",
    "articles = articles[[\"article_id\",\"doc\",\"gender_group\", \"price\", \"rebuy_count\", \"age_bin\", \"prod_sold_count\"]]\n",
    "weekly_sales = transactions.groupby([\"article_id\", \"week\"]).agg({\"customer_id\":\"count\"}).reset_index()\n",
    "last_week_sales = weekly_sales[weekly_sales.week == weekly_sales.week.max()]\n",
    "weekly_sales = weekly_sales.merge(last_week_sales[[\"article_id\",\"customer_id\"]], on=[\"article_id\"], how=\"inner\")\n",
    "weekly_sales[\"quotient\"] = weekly_sales.customer_id_y / weekly_sales.customer_id_x\n",
    "articles = articles.merge(weekly_sales[[\"article_id\",\"quotient\"]], on=\"article_id\", how=\"inner\")\n",
    "\n",
    "del article_info, most_solds, weekly_sales, last_week_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers[\"doc\"] = customers.article_id.apply(lambda x: list(set([i for i in x])))\n",
    "temp_dict = {}\n",
    "for i,row in articles.iterrows():\n",
    "    temp_dict[row.article_id] = row.doc\n",
    "\n",
    "customers[\"doc\"] = customers.doc.apply(lambda x:  \". \".join([temp_dict[i] for i in x]))\n",
    "del temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>FN</th>\n",
       "      <th>Active</th>\n",
       "      <th>club_member_status</th>\n",
       "      <th>fashion_news_frequency</th>\n",
       "      <th>age</th>\n",
       "      <th>age_bin</th>\n",
       "      <th>article_id</th>\n",
       "      <th>week</th>\n",
       "      <th>gender_group</th>\n",
       "      <th>rebuy_count</th>\n",
       "      <th>price</th>\n",
       "      <th>numberOfArticles</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>49.0</td>\n",
       "      <td>3</td>\n",
       "      <td>[568601043, 795440001]</td>\n",
       "      <td>[36, 12]</td>\n",
       "      <td>woman</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032610</td>\n",
       "      <td>2</td>\n",
       "      <td>juan. vest top. garment upper body. solid. bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[826211002, 811927004, 811927004, 811925005, 8...</td>\n",
       "      <td>[28, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 1...</td>\n",
       "      <td>woman</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.024277</td>\n",
       "      <td>15</td>\n",
       "      <td>lazer razer brief. swimwear bottom. swimwear. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[794321007, 852643003, 852643001]</td>\n",
       "      <td>[37, 16, 16]</td>\n",
       "      <td>man</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037271</td>\n",
       "      <td>3</td>\n",
       "      <td>pez sweater. sweater. garment upper body. soli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id   FN  Active  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...  0.0     0.0   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...  0.0     0.0   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0.0     0.0   \n",
       "\n",
       "   club_member_status  fashion_news_frequency   age  age_bin  \\\n",
       "0                   1                       1  49.0        3   \n",
       "1                   1                       1  25.0        1   \n",
       "2                   1                       1  24.0        1   \n",
       "\n",
       "                                          article_id  \\\n",
       "0                             [568601043, 795440001]   \n",
       "1  [826211002, 811927004, 811927004, 811925005, 8...   \n",
       "2                  [794321007, 852643003, 852643001]   \n",
       "\n",
       "                                                week gender_group  \\\n",
       "0                                           [36, 12]        woman   \n",
       "1  [28, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 1...        woman   \n",
       "2                                       [37, 16, 16]          man   \n",
       "\n",
       "   rebuy_count     price  numberOfArticles  \\\n",
       "0          0.0  0.032610                 2   \n",
       "1          0.2  0.024277                15   \n",
       "2          0.0  0.037271                 3   \n",
       "\n",
       "                                                 doc  \n",
       "0  juan. vest top. garment upper body. solid. bla...  \n",
       "1  lazer razer brief. swimwear bottom. swimwear. ...  \n",
       "2  pez sweater. sweater. garment upper body. soli...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>doc</th>\n",
       "      <th>gender_group</th>\n",
       "      <th>price</th>\n",
       "      <th>rebuy_count</th>\n",
       "      <th>age_bin</th>\n",
       "      <th>prod_sold_count</th>\n",
       "      <th>quotient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108775044</td>\n",
       "      <td>strap top. vest top. garment upper body. solid...</td>\n",
       "      <td>woman</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108775044</td>\n",
       "      <td>strap top. vest top. garment upper body. solid...</td>\n",
       "      <td>woman</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108775044</td>\n",
       "      <td>strap top. vest top. garment upper body. solid...</td>\n",
       "      <td>woman</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108775044</td>\n",
       "      <td>strap top. vest top. garment upper body. solid...</td>\n",
       "      <td>woman</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>108775044</td>\n",
       "      <td>strap top. vest top. garment upper body. solid...</td>\n",
       "      <td>woman</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                                doc gender_group  \\\n",
       "0   108775044  strap top. vest top. garment upper body. solid...        woman   \n",
       "1   108775044  strap top. vest top. garment upper body. solid...        woman   \n",
       "2   108775044  strap top. vest top. garment upper body. solid...        woman   \n",
       "3   108775044  strap top. vest top. garment upper body. solid...        woman   \n",
       "4   108775044  strap top. vest top. garment upper body. solid...        woman   \n",
       "\n",
       "      price  rebuy_count  age_bin  prod_sold_count  quotient  \n",
       "0  0.007931     0.096774        1               15  3.000000  \n",
       "1  0.007931     0.096774        1               15  0.750000  \n",
       "2  0.007931     0.096774        1               15  1.500000  \n",
       "3  0.007931     0.096774        1               15  0.333333  \n",
       "4  0.007931     0.096774        1               15  0.142857  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Based Corpus Data Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df1 = articles[[\"article_id\",\"doc\"]].rename(columns={\"article_id\":\"doc_id\"}).copy()\n",
    "articles.drop(columns=[\"doc\"], inplace=True)\n",
    "doc_df2 = customers[[\"customer_id\",\"doc\"]].rename(columns={\"customer_id\":\"doc_id\"}).copy()\n",
    "customers.drop(columns=[\"doc\"], inplace=True)\n",
    "doc_df1[\"type\"] = \"product\"\n",
    "doc_df2[\"type\"] = \"customer\"\n",
    "doc_df = doc_df1.append(doc_df2)\n",
    "doc_df.to_csv(path+\"corpus.csv\", index=False)\n",
    "\n",
    "del doc_df, doc_df1, doc_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collabrative Data Creating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last_week = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions[[\"customer_id\",\"article_id\",\"week\"]].to_pickle(\"data/transactions.pkl\")\n",
    "del transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers saved for prediction...\n"
     ]
    }
   ],
   "source": [
    "customers.FN = customers.FN.astype('category').cat.codes\n",
    "customers.Active = customers.Active.astype('category').cat.codes\n",
    "customers.club_member_status = customers.club_member_status.astype('category').cat.codes\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.astype('category').cat.codes\n",
    "customers.age_bin = customers.age_bin.astype('category').cat.codes\n",
    "customers.gender_group = customers.gender_group.astype('category').cat.codes\n",
    "customers.age = (customers.age - customers.age.min()) / (customers.age.max() - customers.age.min())\n",
    "customers.numberOfArticles = (customers.numberOfArticles - customers.numberOfArticles.min()) / (customers.numberOfArticles.max() - customers.numberOfArticles.min())\n",
    "customers = customers.drop(columns=[\"article_id\", \"week\"])\n",
    "customers.drop_duplicates(subset=[\"customer_id\"], inplace=True)\n",
    "customers.to_pickle(path+\"customers.pkl\")\n",
    "print(\"Customers saved for prediction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles saved for prediction...\n"
     ]
    }
   ],
   "source": [
    "articles.gender_group = articles.gender_group.astype('category').cat.codes\n",
    "articles.age_bin = articles.age_bin.astype('category').cat.codes\n",
    "articles.price = (articles.price - articles.price.min()) / (articles.price.max() - articles.price.min())\n",
    "articles.rebuy_count = (articles.rebuy_count - articles.rebuy_count.min()) / (articles.rebuy_count.max() - articles.rebuy_count.min())\n",
    "articles.prod_sold_count = (articles.prod_sold_count - articles.prod_sold_count.min()) / (articles.prod_sold_count.max() - articles.prod_sold_count.min())\n",
    "articles.quotient = (articles.quotient - articles.quotient.min()) / (articles.quotient.max() - articles.quotient.min())\n",
    "articles.drop_duplicates(subset=[\"article_id\"], inplace=True)\n",
    "articles.to_pickle(path+\"articles.pkl\")\n",
    "print(\"Articles saved for prediction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer history info saved for prediction...\n"
     ]
    }
   ],
   "source": [
    "transactions = pd.read_pickle(\"data/transactions.pkl\")\n",
    "transactions.article_id = transactions.article_id.astype(int)\n",
    "transactions.week = transactions.week.astype(int)\n",
    "customer_hist_info = transactions.groupby([\"customer_id\",\"article_id\"]).agg({\"article_id\":\"count\", \"week\":[\"max\",\"min\"]}).reset_index()\n",
    "customer_hist_info.columns = list(map(lambda x: ''.join(x), customer_hist_info.columns))\n",
    "customer_hist_info[\"avg_purchase_time\"] = customer_hist_info.apply(lambda x: x.weekmax - x.weekmin / (x.article_idcount-1) if x.article_idcount > 1 else 0, axis=1)\n",
    "customer_hist_info = customer_hist_info.rename(columns={\"article_idcount\":\"same_prod_rebuy_count\", \"weekmax\":\"time_passed_last_purchase\"}).reset_index()\n",
    "customer_hist_info = customer_hist_info.drop(columns=[\"weekmin\", \"index\"])\n",
    "customer_hist_info.drop_duplicates(subset=[\"customer_id\"], inplace=True)\n",
    "customer_hist_info.to_pickle(path+\"customer_hist.pkl\")\n",
    "print(\"Customer history info saved for prediction...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Data Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask import dataframe as dd\n",
    "\n",
    "path = 'data/ensemble_train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_pickle(\"data/ensemble_train/customers.pkl\")\n",
    "customers.drop_duplicates(subset=[\"customer_id\"], inplace=True)\n",
    "articles = pd.read_pickle(\"data/ensemble_train/articles.pkl\")\n",
    "articles.drop_duplicates(subset=[\"article_id\"], inplace=True)\n",
    "customer_hist = pd.read_pickle(\"data/ensemble_train/customer_hist.pkl\")\n",
    "customer_hist.drop_duplicates(subset=[\"customer_id\", \"article_id\"], inplace=True)\n",
    "last_week = customer_hist.time_passed_last_purchase.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dd.read_csv('data/transactions_train.csv', dtype={'article_id': str, 'customer_id': str, 'article_id': int})[[\"customer_id\", \"article_id\", \"t_dat\"]]\n",
    "data = data[data.t_dat >= '2020-09-15'].compute()\n",
    "data[\"label\"] = 1.0\n",
    "data.drop_duplicates(subset=[\"customer_id\", \"article_id\"], inplace=True)\n",
    "data.drop(columns=[\"t_dat\"], inplace=True)\n",
    "article_id_list = list(articles.article_id.values)\n",
    "data = data[data.article_id.isin(article_id_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_info = data.groupby([\"customer_id\"]).agg({\"article_id\": lambda x: list(set(x))}).reset_index()\n",
    "group_info[\"prod_num\"] = group_info.article_id.apply(lambda x: len(x))\n",
    "customer_id_list = data.customer_id.to_list()\n",
    "prod_id_list = data.article_id.to_list()\n",
    "label_list = data.label.to_list()\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71519: %100.0"
     ]
    }
   ],
   "source": [
    "for i, row in group_info.iterrows():\n",
    "    temp_articles = random.sample(article_id_list, 550)\n",
    "    step = 0\n",
    "    for id in temp_articles:\n",
    "        if id not in row.article_id:\n",
    "            customer_id_list.append(row.customer_id)\n",
    "            label_list.append(0.0)\n",
    "            prod_id_list.append(id)\n",
    "            step += 1\n",
    "        if step >= 500:\n",
    "            break\n",
    "    print('\\r' + f'{i}: %{round(100*i/group_info.shape[0], 2)}', end='')\n",
    "\n",
    "data = pd.DataFrame({\"customer_id\": customer_id_list, \"article_id\": prod_id_list, \"label\": label_list, \"week\": last_week+1})\n",
    "data = dd.from_pandas(data, npartitions= 32)\n",
    "articles = dd.from_pandas(articles, npartitions= 4)\n",
    "customers = dd.from_pandas(customers, npartitions= 4)\n",
    "customer_hist = dd.from_pandas(customer_hist, npartitions= 4)\n",
    "del label_list, prod_id_list, customer_id_list, group_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(articles.rename(columns={\"age_bin\":\"prod_age_bin\", \"gender_group\":\"prod_gender_group\", \\\n",
    "                                       \"rebuy_count\":\"prod_rebuy_count\",\"price\":\"prod_avg_price\"}), on=\"article_id\", how=\"inner\")\n",
    "data = data.merge(customers.rename(columns={\"age_bin\":\"customer_age_bin\", \"gender_group\":\"customer_gender_group\",\\\n",
    "                                        \"rebuy_count\":\"customer_rebuy_count\",\"price\":\"customer_avg_price\", \\\n",
    "                                        \"article_id\":\"article_hist\", \"week\":\"week_hist\"}), on=\"customer_id\", how=\"inner\")\n",
    "data = data.merge(customer_hist, on=[\"customer_id\",\"article_id\"], how=\"left\")\n",
    "data.same_prod_rebuy_count = data.same_prod_rebuy_count.fillna(0)\n",
    "data.avg_purchase_time = data.avg_purchase_time.fillna(0)\n",
    "data.time_passed_last_purchase = data.time_passed_last_purchase.fillna(39 - 29) # 6 mounths is nearly 29 week\n",
    "data.time_passed_last_purchase = data.apply(lambda x: x.week - x.time_passed_last_purchase, meta=(\"time_passed_last_purchase\",\"int\"), axis=1)\n",
    "\n",
    "del articles, customers, customer_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_911/3647925529.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"week\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"ensemble_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, filename, **kwargs)\u001b[0m\n\u001b[1;32m   1624\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m     def to_sql(\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/dask/dataframe/io/csv.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(df, filename, single_file, encoding, mode, name_function, compression, compute, scheduler, storage_options, header_first_partition_only, compute_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcompute_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/dask/threaded.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dsk, result, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiprocessingPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     results = get_async(\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/dask/local.py\u001b[0m in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"waiting\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ready\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"running\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0mfire_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                         \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/dask/local.py\u001b[0m in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = data.drop(columns=[\"week\"])\n",
    "data.to_csv(path + \"ensemble_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Data Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from dask import dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_pickle(\"data/ensemble/customers.pkl\")\n",
    "customers.drop_duplicates(subset=[\"customer_id\"], inplace=True)\n",
    "articles = pd.read_pickle(\"data/ensemble/articles.pkl\")\n",
    "articles.drop_duplicates(subset=[\"article_id\"], inplace=True)\n",
    "customer_hist = pd.read_pickle(\"data/ensemble/customer_hist.pkl\")\n",
    "customer_hist.drop_duplicates(subset=[\"customer_id\", \"article_id\"], inplace=True)\n",
    "article_ids = articles.article_id.values.tolist()\n",
    "customer_ids = customers.customer_id.values\n",
    "customers = cudf.DataFrame.from_pandas(customers)\n",
    "customer_hist = cudf.DataFrame.from_pandas(customer_hist)\n",
    "articles = cudf.DataFrame.from_pandas(articles)\n",
    "batch_size = 512\n",
    "article_ids = article_ids * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192: %1.22"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/dataframe.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, columns, header, index, line_terminator, chunksize, encoding, compression, **kwargs)\u001b[0m\n\u001b[1;32m   5560\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5562\u001b[0;31m         return csv.to_csv(\n\u001b[0m\u001b[1;32m   5563\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5564\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/io/csv.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(df, path_or_buf, sep, na_rep, columns, header, index, line_terminator, chunksize, encoding, compression, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m             )\n\u001b[1;32m    203\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         libcudf.csv.write_csv(\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_list = []\n",
    "loop_size = len(customer_ids) + batch_size\n",
    "for batch_i in range(batch_size, loop_size, batch_size):\n",
    "    customer_ids_batch = customer_ids[0:batch_size]\n",
    "    customer_ids_batch = np.repeat(customer_ids_batch, len(article_ids)/batch_size)\n",
    "    df = cudf.DataFrame({\"customer_id\": customer_ids_batch, \"article_id\": article_ids, \"week\": 39})\n",
    "    df = df.merge(articles.rename(columns={\"age_bin\":\"prod_age_bin\", \"gender_group\":\"prod_gender_group\", \\\n",
    "                                           \"rebuy_count\":\"prod_rebuy_count\",\"price\":\"prod_avg_price\"}), on=\"article_id\", how=\"inner\")\n",
    "    df = df.merge(customers.rename(columns={\"age_bin\":\"customer_age_bin\", \"gender_group\":\"customer_gender_group\",\\\n",
    "                                            \"rebuy_count\":\"customer_rebuy_count\",\"price\":\"customer_avg_price\", \\\n",
    "                                            \"article_id\":\"article_hist\", \"week\":\"week_hist\"}), on=\"customer_id\", how=\"inner\")\n",
    "    df = df.merge(customer_hist, on=[\"customer_id\",\"article_id\"], how=\"left\")\n",
    "    df.same_prod_rebuy_count = df.same_prod_rebuy_count.fillna(0)\n",
    "    df.avg_purchase_time = df.avg_purchase_time.fillna(0)\n",
    "    df.time_passed_last_purchase = df.time_passed_last_purchase.fillna(39 - 29) # 6 mounths is nearly 29 week\n",
    "    df.time_passed_last_purchase = df.apply(lambda x: x.week - x.time_passed_last_purchase)\n",
    "    df.to_csv(f\"data/ensemble/final_data/{batch_i}.csv\")\n",
    "    del df\n",
    "    gc.collect()\n",
    "    print('\\r' + f'{batch_i}: %{round(100*batch_i/loop_size, 2)}', end='')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad61a90f8b8ec2120f8b8d3efc4267caee6017c89f89db236bf61b71644f2c7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
