{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from dask import dataframe as dd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = dd.read_csv('data/transactions_train.csv', dtype={'article_id': int, 'customer_id': str})\n",
    "if train:\n",
    "    transactions = transactions[(transactions.t_dat >= '2020-03-17') & (transactions.t_dat <= '2020-09-15')]\n",
    "    path = 'data/ensemble_train/'\n",
    "else:\n",
    "    path = 'data/ensemble/'\n",
    "    transactions = transactions[transactions.t_dat >= '2020-03-24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.t_dat = dd.to_datetime(transactions.t_dat) - timedelta(2)\n",
    "transactions[\"week\"] = transactions.t_dat.dt.isocalendar().week\n",
    "transactions.week = transactions.week.astype(int)\n",
    "transactions = transactions.compute()\n",
    "transactions[\"rebuy_count\"] = transactions.groupby([\"customer_id\", \"article_id\"]).cumcount().astype(int)\n",
    "transactions[\"rebuy_count\"] = transactions.rebuy_count.apply(lambda x: x -1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_solds = transactions[transactions.week > transactions.week.max()-3].groupby([\"article_id\"]).agg({\"article_id\":\"count\"})\\\n",
    "                                   .rename(columns={\"article_id\":\"_count\"}).reset_index()\\\n",
    "                                   .sort_values('_count', ascending=False)\n",
    "most_solds = most_solds.head(10000)\n",
    "transactions = transactions[transactions.article_id.isin(most_solds.article_id)]\n",
    "last_week_articles = transactions[transactions.week == transactions.week.max()].article_id.unique()\n",
    "articles = pd.read_csv(\"data/articles.csv\", dtype={'article_id': int})\n",
    "articles.drop_duplicates(subset=['article_id'], inplace=True)\n",
    "articles = articles[(articles.article_id.isin(most_solds.article_id)) & (articles.article_id.isin(last_week_articles))]\n",
    "del most_solds, last_week_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_customers = transactions.customer_id.unique()\n",
    "customers = pd.read_csv(\"data/customers.csv\", dtype={'customer_id': str})\n",
    "customers.drop_duplicates(subset=['customer_id'], inplace=True)\n",
    "customers = customers[customers.customer_id.isin(active_customers)]\n",
    "del active_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_classification(age):\n",
    "    if age < 19:\n",
    "        return 0\n",
    "    elif age < 29:\n",
    "        return 1\n",
    "    elif age < 49:\n",
    "        return 2\n",
    "    elif age < 59:\n",
    "        return 3\n",
    "    elif age < 69:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "customers[\"age\"] = customers.age.fillna(np.mean(customers.age))\n",
    "customers[\"age_bin\"] = customers.age.map(gender_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_classification(section_name):\n",
    "    if \"womens\" in section_name or \"girl\" in section_name or \"ladies\" in section_name:\n",
    "        return \"woman\"\n",
    "    elif \"men\" in section_name or \"boy\" in section_name or \"boys\" in section_name:\n",
    "        return \"man\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "articles.section_name = articles.section_name.map(lambda x: x.lower())\n",
    "articles[\"gender_group\"] = articles.section_name.apply(gender_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = transactions.merge(articles[[\"article_id\", \"gender_group\"]], on=\"article_id\", how=\"inner\")\n",
    "transactions = transactions.merge(customers[[\"customer_id\",\"age_bin\", \"age\"]], on=\"customer_id\", how=\"inner\")\n",
    "prod_avg_age = transactions.groupby(by=\"article_id\").agg({\"age\":[\"mean\", \"std\"]}).reset_index()\n",
    "prod_avg_age.columns = list(map(lambda x: '_'.join(x), prod_avg_age.columns))\n",
    "prod_avg_age.rename(columns={\"article_id_\":\"article_id\", \"_std\":\"std_age\"}, inplace=True)\n",
    "articles = articles.merge(prod_avg_age.rename(columns={\"article_id_\":\"article_id\"}), on=\"article_id\", how=\"inner\")\n",
    "del prod_avg_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_hist = transactions.groupby(by=\"customer_id\").agg({\"article_id\": lambda x: list(x.values), \"week\": lambda x: list(x.values), \"gender_group\": lambda x : x.mode().iloc[0],\"rebuy_count\": \"mean\"}).reset_index()\n",
    "customers = customers.merge(customer_hist, on=\"customer_id\", how=\"left\")\n",
    "customers.article_id = customers.article_id.fillna(\"\").apply(list)\n",
    "customers.gender_group = customers.gender_group.fillna(\"other\")\n",
    "transactions.price = transactions.price.fillna(transactions.price.mean())\n",
    "prod_price = transactions.groupby(\"customer_id\").agg({\"price\":\"mean\"}).rename(columns={\"customer_id\":\"price_\"}).reset_index()\n",
    "prod_price.columns = list(map(''.join, prod_price.columns.values))\n",
    "customers = customers.merge(prod_price, on=\"customer_id\", how=\"inner\")\n",
    "customers[\"freq_week\"] = customers.week.apply(lambda x: (max(x)-min(x))/len(x))\n",
    "customers[\"different_week\"] = customers.week.apply(lambda x: len(set(x)))\n",
    "del customer_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_customer_hist(week, article_id):\n",
    "    customer_hist = np.asarray(sorted(zip(week, article_id), reverse=True))\n",
    "    return customer_hist[:,0], customer_hist[:,1]\n",
    "\n",
    "customers[[\"week\", \"article_id\"]] = customers.apply(lambda x: sort_customer_hist(x.week, x.article_id), axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.FN = customers.FN.fillna(0)\n",
    "customers.Active = customers.Active.fillna(0)\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.fillna(\"not_regular\")\n",
    "customers.club_member_status = customers.club_member_status.fillna(\"no-info\")\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.apply(lambda x: \"not_regular\" if x == \"NONE\" or x == \"None\" else x)\n",
    "customers[\"numberOfArticles\"] = customers.apply(lambda x: len(x.article_id), axis=1)\n",
    "customers = customers.drop(columns=[\"postal_code\"])\n",
    "customers.sort_values(by=\"customer_id\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_fashion_news(name):\n",
    "    return 1 if name == 'not_regular' else 0\n",
    "def map_club_member(name):\n",
    "    return 1 if name == 'ACTIVE' else 0\n",
    "\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.map(map_fashion_news)\n",
    "customers.club_member_status = customers.club_member_status.map(map_club_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article2doc(x):\n",
    "    def clean_doc(text):\n",
    "        unwanted_chars = ['1','2','3','4','5','6','7','8','9','(',')','[',']']\n",
    "        for chr in unwanted_chars:\n",
    "            text = text.replace(chr, '')\n",
    "        return text\n",
    "\n",
    "    doc =  '. '.join([x.prod_name, x.product_type_name, x.product_group_name, x.graphical_appearance_name, x.colour_group_name,\\\n",
    "                      x.perceived_colour_value_name, x.perceived_colour_master_name, x.department_name, x.index_name, x.index_group_name,\\\n",
    "                      x.section_name, x.garment_group_name, str(x.detail_desc)])[:-1]\n",
    "    return(clean_doc(doc.lower()))\n",
    "\n",
    "articles[\"doc\"] = articles.apply(article2doc, axis=1)\n",
    "article_info = transactions.groupby([\"article_id\"]).agg({\"price\":\"mean\", \"rebuy_count\":\"mean\", \"age_bin\": lambda x : x.mode().iloc[0]}).reset_index()\n",
    "articles = articles.merge(article_info, on=\"article_id\", how=\"inner\")\n",
    "most_solds = transactions[transactions.week > transactions.week.max()-3].groupby([\"article_id\"]).agg({\"customer_id\":\"count\"})\\\n",
    "                                      .rename(columns={\"customer_id\":\"prod_sold_count\"}).reset_index()\n",
    "articles = articles.merge(most_solds, on=\"article_id\", how=\"inner\")\n",
    "articles = articles[[\"article_id\",\"doc\",\"gender_group\", \"price\", \"rebuy_count\", \"age_bin\", \"prod_sold_count\", \"age_std\", \"age_mean\"]]\n",
    "weekly_sales = transactions.groupby([\"article_id\", \"week\"]).agg({\"customer_id\":\"count\"}).reset_index()\n",
    "last_week_sales = weekly_sales[weekly_sales.week == weekly_sales.week.max()]\n",
    "weekly_sales = weekly_sales.merge(last_week_sales[[\"article_id\",\"customer_id\"]], on=[\"article_id\"], how=\"inner\")\n",
    "weekly_sales[\"quotient\"] = weekly_sales.customer_id_y / weekly_sales.customer_id_x\n",
    "articles = articles.merge(weekly_sales[[\"article_id\",\"quotient\"]], on=\"article_id\", how=\"inner\")\n",
    "\n",
    "del article_info, most_solds, weekly_sales, last_week_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers[\"doc\"] = customers.article_id.apply(lambda x: list(set([i for i in x])))\n",
    "temp_dict = {}\n",
    "for i,row in articles.iterrows():\n",
    "    temp_dict[row.article_id] = row.doc\n",
    "\n",
    "customers[\"doc\"] = customers.doc.apply(lambda x:  \". \".join([temp_dict[i] for i in x]))\n",
    "del temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Based Corpus Data Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df1 = articles[[\"article_id\",\"doc\"]].rename(columns={\"article_id\":\"doc_id\"}).copy()\n",
    "articles.drop(columns=[\"doc\"], inplace=True)\n",
    "doc_df2 = customers[[\"customer_id\",\"doc\"]].rename(columns={\"customer_id\":\"doc_id\"}).copy()\n",
    "customers.drop(columns=[\"doc\"], inplace=True)\n",
    "doc_df1[\"type\"] = \"product\"\n",
    "doc_df2[\"type\"] = \"customer\"\n",
    "doc_df = doc_df1.append(doc_df2)\n",
    "doc_df.to_csv(path+\"corpus.csv\", index=False)\n",
    "\n",
    "del doc_df, doc_df1, doc_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collabrative Data Creating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last_week = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions[[\"customer_id\",\"article_id\",\"week\"]].to_pickle(path+\"transactions.pkl\")\n",
    "del transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.FN = customers.FN.astype('category').cat.codes\n",
    "customers.Active = customers.Active.astype('category').cat.codes\n",
    "customers.club_member_status = customers.club_member_status.astype('category').cat.codes\n",
    "customers.fashion_news_frequency = customers.fashion_news_frequency.astype('category').cat.codes\n",
    "customers.age_bin = customers.age_bin.astype('category').cat.codes\n",
    "customers.gender_group = customers.gender_group.astype('category').cat.codes\n",
    "customers.drop_duplicates(subset=[\"customer_id\"], inplace=True)\n",
    "customers.age = (customers.age - customers.age.min()) / (customers.age.max() - customers.age.min())\n",
    "customers.numberOfArticles = (customers.numberOfArticles - customers.numberOfArticles.min()) / (customers.numberOfArticles.max() - customers.numberOfArticles.min())\n",
    "customers.different_week = (customers.different_week - customers.different_week.min()) / (customers.different_week.max() - customers.different_week.min())\n",
    "customers.freq_week = (customers.freq_week - customers.freq_week.min()) / (customers.freq_week.max() - customers.freq_week.min())\n",
    "customers = customers.drop(columns=[\"article_id\", \"week\"])\n",
    "customers = customers.reset_index().rename(columns = {'index':'customer_index'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.age = customers.age.astype(np.float32)\n",
    "customers.freq_week = customers.freq_week.astype(np.float32)\n",
    "customers.different_week = customers.different_week.astype(np.float32)\n",
    "customers.numberOfArticles = customers.numberOfArticles.astype(np.float32)\n",
    "customers.drop(columns=[\"FN\",\"Active\",\"club_member_status\",\"fashion_news_frequency\",\"age_bin\",\"gender_group\",\"price\",\"rebuy_count\"], inplace=True)\n",
    "customers.drop_duplicates(subset=[\"customer_id\"], inplace=True)\n",
    "customers.to_pickle(path+\"customers.pkl\")\n",
    "print(\"Customers saved for prediction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.gender_group = articles.gender_group.astype('category').cat.codes\n",
    "articles.age_bin = articles.age_bin.astype('category').cat.codes\n",
    "articles.price = (articles.price - articles.price.min()) / (articles.price.max() - articles.price.min())\n",
    "articles.age_mean = (articles.age_mean - articles.age_mean.min()) / (articles.age_mean.max() - articles.age_mean.min())\n",
    "articles.age_std = (articles.age_std - articles.age_std.min()) / (articles.age_std.max() - articles.age_std.min())\n",
    "articles.rebuy_count = (articles.rebuy_count - articles.rebuy_count.min()) / (articles.rebuy_count.max() - articles.rebuy_count.min())\n",
    "articles.prod_sold_count = (articles.prod_sold_count - articles.prod_sold_count.min()) / (articles.prod_sold_count.max() - articles.prod_sold_count.min())\n",
    "articles.quotient = (articles.quotient - articles.quotient.min()) / (articles.quotient.max() - articles.quotient.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.rebuy_count = articles.rebuy_count.astype(np.float32)\n",
    "articles.prod_sold_count = articles.prod_sold_count.astype(np.float32)\n",
    "articles.age_std = articles.age_std.astype(np.float32)\n",
    "articles.age_mean = articles.age_mean.astype(np.float32)\n",
    "articles.quotient = articles.quotient.astype(np.float32)\n",
    "articles.drop(columns=[\"gender_group\", \"price\", \"age_bin\"],inplace=True)\n",
    "articles.drop_duplicates(subset=[\"article_id\"], inplace=True)\n",
    "articles.to_pickle(path+\"articles.pkl\")\n",
    "print(\"Articles saved for prediction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_pickle(\"data/transactions.pkl\")\n",
    "transactions.article_id = transactions.article_id.astype(np.int64)\n",
    "transactions.week = transactions.week.astype(int)\n",
    "customer_hist_info = transactions.groupby([\"customer_id\",\"article_id\"]).agg({\"article_id\":\"count\", \"week\":[\"max\",\"min\"]}).reset_index()\n",
    "customer_hist_info.columns = list(map(lambda x: ''.join(x), customer_hist_info.columns))\n",
    "customer_hist_info[\"avg_purchase_time\"] = customer_hist_info.apply(lambda x: x.weekmax - x.weekmin / (x.article_idcount-1) if x.article_idcount > 1 else 0, axis=1)\n",
    "customer_hist_info = customer_hist_info.rename(columns={\"article_idcount\":\"same_prod_rebuy_count\", \"weekmax\":\"time_passed_last_purchase\"}).reset_index()\n",
    "customer_hist_info = customer_hist_info.drop(columns=[\"weekmin\", \"index\"])\n",
    "customer_hist_info.same_prod_rebuy_count = (customer_hist_info.same_prod_rebuy_count - customer_hist_info.same_prod_rebuy_count.min()) / (customer_hist_info.same_prod_rebuy_count.max() - customer_hist_info.same_prod_rebuy_count.min())\n",
    "customer_hist_info.time_passed_last_purchase = (customer_hist_info.time_passed_last_purchase - customer_hist_info.time_passed_last_purchase.min()) / (customer_hist_info.time_passed_last_purchase.max() - customer_hist_info.time_passed_last_purchase.min())\n",
    "customer_hist_info.avg_purchase_time = (customer_hist_info.avg_purchase_time - customer_hist_info.avg_purchase_time.min()) / (customer_hist_info.avg_purchase_time.max() - customer_hist_info.avg_purchase_time.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_hist_info.same_prod_rebuy_count = customer_hist_info.same_prod_rebuy_count.astype(np.float32)\n",
    "customer_hist_info.time_passed_last_purchase = customer_hist_info.time_passed_last_purchase.astype(np.float32)\n",
    "customer_hist_info.avg_purchase_time = customer_hist_info.avg_purchase_time.astype(np.float32)\n",
    "customer_hist_info.drop_duplicates(subset=[\"customer_id\", \"article_id\"], inplace=True)\n",
    "customer_hist_info.to_pickle(path+\"customer_hist.pkl\")\n",
    "print(\"Customer history info saved for prediction...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Data Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from dask import dataframe as dd\n",
    "\n",
    "path = 'data/ensemble_train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_pickle(\"data/ensemble_train/customers.pkl\")\n",
    "customers.drop_duplicates(subset=[\"customer_id\"], inplace=True)\n",
    "articles = pd.read_pickle(\"data/ensemble_train/articles.pkl\")\n",
    "articles.drop_duplicates(subset=[\"article_id\"], inplace=True)\n",
    "customer_hist = pd.read_pickle(\"data/ensemble_train/customer_hist.pkl\")\n",
    "customer_hist.drop_duplicates(subset=[\"customer_id\", \"article_id\"], inplace=True)\n",
    "last_week = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dd.read_csv('data/transactions_train.csv', dtype={'article_id': str, 'customer_id': str, 'article_id': int})[[\"customer_id\", \"article_id\", \"t_dat\"]]\n",
    "data = data[data.t_dat >= '2020-09-15'].compute()\n",
    "data[\"label\"] = 1.0\n",
    "data.drop_duplicates(subset=[\"customer_id\", \"article_id\"], inplace=True)\n",
    "data.drop(columns=[\"t_dat\"], inplace=True)\n",
    "article_id_list = list(articles.article_id.values)\n",
    "customer_id_list = list(customers.customer_id.values)\n",
    "data = data[data.article_id.isin(article_id_list)]\n",
    "data = data[data.customer_id.isin(customer_id_list)]\n",
    "\n",
    "del customer_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_info = data.groupby([\"customer_id\"]).agg({\"article_id\": lambda x: list(set(x))}).reset_index()\n",
    "group_info[\"prod_num\"] = group_info.article_id.apply(lambda x: len(x))\n",
    "customer_id_df_list = data.customer_id.to_list()\n",
    "article_id_df_list = data.article_id.to_list()\n",
    "label_list = data.label.to_list()\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58736: %100.0"
     ]
    }
   ],
   "source": [
    "for i, row in group_info.iterrows():\n",
    "    temp_articles = random.sample(article_id_list, 1000)\n",
    "    step = 0\n",
    "    for id in temp_articles:\n",
    "        if id not in row.article_id:\n",
    "            customer_id_df_list.append(row.customer_id)\n",
    "            label_list.append(0.0)\n",
    "            article_id_df_list.append(id)\n",
    "            step += 1\n",
    "        if step >= 1000:\n",
    "            break\n",
    "    print('\\r' + f'{i}: %{round(100*i/group_info.shape[0], 2)}', end='')\n",
    "\n",
    "data = pd.DataFrame({\"customer_id\": customer_id_df_list, \"article_id\": article_id_df_list, \"label\": label_list, \"week\": last_week+1})\n",
    "data.sort_values(by=[\"customer_id\"], inplace=True)\n",
    "del label_list, article_id_df_list, customer_id_df_list, group_info, article_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_encoding = {k:v for v,k in enumerate(customers.customer_id)}\n",
    "product_encoding = {k:v for v,k in enumerate(articles.article_id)}\n",
    "d2v = h5py.File('personalization\\ensemble_train\\d2vf.h5', \"r\")[\"d2v\"]\n",
    "dl = h5py.File('personalization\\ensemble_train\\dl.h5', \"r\")[\"dl\"]\n",
    "tf_idf = h5py.File('personalization/ensemble_train/tf_idf.h5', \"r\")[\"tf_idf\"]\n",
    "nmf = h5py.File('personalization/ensemble_train/nmf.h5', \"r\")[\"nmf\"]\n",
    "lda = h5py.File('personalization/ensemble_train/lda.h5', \"r\")[\"lda\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58737: %100.0"
     ]
    }
   ],
   "source": [
    "group_info = data.groupby([\"customer_id\"])\n",
    "d2v_df_list = []\n",
    "dl_df_list = []\n",
    "tf_idf_df_list = []\n",
    "nmf_df_list = []\n",
    "lda_df_list = []\n",
    "number_of_customers = len(data.customer_id.unique())\n",
    "step = 1\n",
    "batch = 4096\n",
    "customer_indexes = []\n",
    "prod_indexes = []\n",
    "for c_id, df in group_info:\n",
    "    customer_indexes.append(customer_encoding[c_id])\n",
    "    prod_indexes.append(list(map(lambda x: product_encoding[x], df.article_id.values)))\n",
    "    if step % batch == 0 or (step+batch) >= number_of_customers:\n",
    "        d2v_vecs = d2v[customer_indexes]\n",
    "        dl_vecs = dl[customer_indexes]\n",
    "        tf_idf_vecs = tf_idf[customer_indexes]\n",
    "        nmf_vecs = nmf[customer_indexes]\n",
    "        lda_vecs = lda[customer_indexes]\n",
    "        for i, p_ids in enumerate(prod_indexes):\n",
    "            d2v_df_list += d2v_vecs[i, p_ids].tolist()\n",
    "            dl_df_list += dl_vecs[i, p_ids].tolist()\n",
    "            tf_idf_df_list += tf_idf_vecs[i, p_ids].tolist()\n",
    "            nmf_df_list += nmf_vecs[i, p_ids].tolist()\n",
    "            lda_df_list += lda_vecs[i, p_ids].tolist()\n",
    "        customer_indexes = []\n",
    "        prod_indexes = []\n",
    "    print('\\r' + f'{step}: %{round(100*step/number_of_customers, 2)}', end='')\n",
    "    step += 1\n",
    "\n",
    "data[\"d2v\"] = d2v_df_list\n",
    "data[\"dl\"] = dl_df_list\n",
    "data[\"tf_idf\"] = tf_idf_df_list\n",
    "data[\"nmf\"] = nmf_df_list\n",
    "data[\"lda\"] = lda_df_list\n",
    "del d2v_df_list, dl_df_list, tf_idf_df_list, group_info, product_encoding, customer_encoding, nmf_df_list, lda_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dd.from_pandas(data, npartitions= 32)\n",
    "articles = dd.from_pandas(articles, npartitions= 4)\n",
    "customers = dd.from_pandas(customers, npartitions= 4)\n",
    "customer_hist = dd.from_pandas(customer_hist, npartitions= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.article_id = articles.article_id.astype(np.int64)\n",
    "data = data.merge(articles.rename(columns={\"rebuy_count\":\"prod_rebuy_count\"}), on=\"article_id\", how=\"inner\")\n",
    "data = data.merge(customers, on=\"customer_id\", how=\"inner\")\n",
    "data = data.merge(customer_hist, on=[\"customer_id\",\"article_id\"], how=\"left\")\n",
    "data.same_prod_rebuy_count = data.same_prod_rebuy_count.fillna(0)\n",
    "data.time_passed_last_purchase = data.time_passed_last_purchase.fillna(1)\n",
    "data.avg_purchase_time = data.avg_purchase_time.fillna(0)\n",
    "\n",
    "\n",
    "del articles, customers, customer_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\00.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\01.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\02.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\03.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\04.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\05.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\06.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\07.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\08.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\09.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\10.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\11.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\12.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\13.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\14.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\15.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\16.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\17.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\18.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\19.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\20.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\21.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\22.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\23.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\24.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\25.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\26.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\27.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\28.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\29.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\30.part',\n",
       " 'd:/workspace/H-M/data/ensemble_train/ensemble_data.csv\\\\31.part']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns=[\"week\"])\n",
    "data = data.sort_values(by=[\"customer_id\"]).reset_index(drop=True)\n",
    "data.to_csv(path + \"ensemble_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0286e36f0b6cc42564d593afe30408db71b090a133f6805167cf9f46adee6ce3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
