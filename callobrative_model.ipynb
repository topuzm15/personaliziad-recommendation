{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from dask import dataframe as dd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = dd.read_csv('data/transactions_train.csv')[[\"customer_id\", \"article_id\"]]\n",
    "customer_purchase_number = transactions.groupby(\"customer_id\").size()\n",
    "customer_purchase_number = customer_purchase_number[customer_purchase_number > 3].compute()\n",
    "transactions = transactions[transactions[\"customer_id\"].isin(customer_purchase_number.keys())].reset_index(drop=True)\n",
    "c_ids = transactions.customer_id.unique()\n",
    "number_of_customer = len(c_ids)\n",
    "customer_encoding = {c_id: i for i, c_id in enumerate(c_ids)}\n",
    "p_ids = transactions.article_id.unique()\n",
    "number_of_products = len(p_ids)\n",
    "product_encoding = {p_id: i for i, p_id in enumerate(p_ids)}\n",
    "with open('model_data/customer_id_encoding.json', 'w') as fp:\n",
    "    json.dump(customer_encoding, fp)\n",
    "\n",
    "with open('model_data/product_id_encoding.json', 'w') as fp:\n",
    "    json.dump(product_encoding, fp)\n",
    "\n",
    "transactions.customer_id = transactions.customer_id.map(customer_encoding)\n",
    "transactions.article_id = transactions.article_id.map(product_encoding)\n",
    "p_ids = list(product_encoding.values())\n",
    "del c_ids, product_encoding, customer_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = transactions.random_split([0.9, 0.1], random_state=43)\n",
    "df = test.merge(train[[\"customer_id\"]], on=[\"customer_id\"], how=\"outer\", indicator=True)\n",
    "train = dd.concat([train, df[df['_merge'] == 'left_only'][[\"customer_id\", \"article_id\"]]], axis=0, ignore_index=True, interleave_partitions=True, ignore_order=True)\n",
    "df = df[df['_merge'] == 'both'][[\"customer_id\"]].drop_duplicates()\n",
    "test = test.merge(df, how=\"inner\", on=\"customer_id\")\n",
    "steps_per_epoch = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(transactions, p_ids):\n",
    "    transactions = transactions.groupby([\"customer_id\"])['article_id']\\\n",
    "                            .apply(lambda x: list(x), meta=(\"article_ids\",object))\\\n",
    "                            .reset_index().compute().drop_duplicates(subset=[\"customer_id\"])\n",
    "\n",
    "    transactions[\"hist_len\"] = transactions.article_ids.apply(lambda x: 128 if len(x)>128 else len(x))\n",
    "    transactions[\"prod_ids\"] = transactions.apply(lambda x: random.sample(x.article_ids, x.hist_len), axis=1)\n",
    "    transactions[\"not_prods\"] = transactions.prod_ids.apply(lambda x: [p_id for p_id in random.sample(p_ids, 256) if p_id not in x][:128])\n",
    "    transactions = transactions[[\"customer_id\", \"prod_ids\", \"not_prods\"]]\n",
    "    return transactions\n",
    "\n",
    "train = prepare_data(train, p_ids)\n",
    "test = prepare_data(test, p_ids)\n",
    "train.to_pickle('data/train.csv', index=False)\n",
    "test.to_pickle('data/test.csv', index=False)\n",
    "print(train.shape, test.shape)\n",
    "del p_ids, transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, data, positive_sample_length, negative_sample_length, batch_size):\n",
    "        self.data = data\n",
    "        self.positive_sample_length = positive_sample_length\n",
    "        self.negative_sample_length = negative_sample_length\n",
    "        self.sample_length = positive_sample_length + negative_sample_length\n",
    "        if batch_size % self.sample_length != 0:\n",
    "            raise ValueError(\"batch_size must be divisible by sum of positive_sample_length and negative_sample_length\")\n",
    "\n",
    "\n",
    "    def user_info_generator(self):\n",
    "        for i, row in self.data[[\"customer_id\",\"prod_ids\",\"not_prods\"]].iterrows():\n",
    "            pids = np.asarray(random.sample(row[\"prod_ids\"], self.positive_sample_length) + random.sample(row[\"not_prods\"], self.negative_sample_length))\n",
    "            labels = np.asarray([1]*self.positive_sample_length + [0]*self.negative_sample_length)\n",
    "            indices = np.arange(self.sample_length)\n",
    "            np.random.shuffle(indices)\n",
    "            yield tf.convert_to_tensor(np.asarray([pids[indices],labels[indices]], dtype=np.int32), dtype=tf.int32)\n",
    "\n",
    "    def user_id_generator(self):\n",
    "        for i, row in self.data[[\"customer_id\"]].iterrows():\n",
    "            customer_id = [row[\"customer_id\"]] * self.sample_length\n",
    "            yield tf.convert_to_tensor(customer_id, dtype=tf.int32)\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.data = shuffle(self.data)\n",
    "\n",
    "class TGenerator (Generator):\n",
    "    def __init__(self, data, positive_sample_length, negative_sample_length, batch_size):\n",
    "        super().__init__(data, positive_sample_length, negative_sample_length, batch_size)\n",
    "        self.mini_batch = int(batch_size / self.sample_length)\n",
    "        self.batch = batch_size\n",
    "        self.user_info_loader = tf.data.Dataset.from_generator(\n",
    "            self.user_info_generator, output_types=tf.int32).batch(self.mini_batch, drop_remainder=True).repeat()\n",
    "        self.user_id_loader = tf.data.Dataset.from_generator(\n",
    "            self.user_id_generator, output_types=tf.int32).batch(self.mini_batch, drop_remainder=True).repeat()\n",
    "\n",
    "    def get(self):\n",
    "        for c_ids, info in zip(self.user_id_loader, self.user_info_loader):\n",
    "            input = tf.stack([tf.reshape(c_ids, self.batch), tf.reshape(info[:,0,:], self.batch)], axis=1)\n",
    "            label = tf.reshape(info[:,1,:], self.batch)\n",
    "            yield input, label\n",
    "\n",
    "class VGenerator (Generator):\n",
    "    def __init__(self, data, positive_sample_length, negative_sample_length, batch_size):\n",
    "        super().__init__(data, positive_sample_length, negative_sample_length, batch_size)\n",
    "        self.mini_batch = int(batch_size / self.sample_length)\n",
    "        self.batch = batch_size\n",
    "        self.user_info_loader = tf.data.Dataset.from_generator(\n",
    "            self.user_info_generator, output_types=tf.int32).batch(self.mini_batch, drop_remainder=True).repeat()\n",
    "        self.user_id_loader = tf.data.Dataset.from_generator(\n",
    "            self.user_id_generator, output_types=tf.int32).batch(self.mini_batch, drop_remainder=True).repeat()\n",
    "\n",
    "    def get(self):\n",
    "        for c_ids, info in zip(self.user_id_loader, self.user_info_loader):\n",
    "            try:\n",
    "                input = tf.stack([tf.reshape(c_ids, self.batch), tf.reshape(info[:,0,:], self.batch)], axis=1)\n",
    "                label = tf.reshape(info[:,1,:], self.batch)\n",
    "                yield input, label\n",
    "            except:\n",
    "                print(\"error\")\n",
    "                yield None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-08 21:45:56.615878: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_data = pd.read_pickle(\"data/train.csv\")\n",
    "test_data = pd.read_pickle(\"data/test.csv\")\n",
    "train_loader = TGenerator(train_data, 1, 1, batch_size)\n",
    "\n",
    "validation_loader = VGenerator(test_data, 1, 1, batch_size)\n",
    "train = train_loader.get()\n",
    "val = validation_loader.get()\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "steps_per_epoch = len(train)\n",
    " = pd.read_pickle(\"data/train.csv\")\n",
    "test_data = pd.read_pickle(\"data/test.csv\")\n",
    "train_loader = TGenerator(train, 1, 1, batch_size)\n",
    "validation_loader = VGenerator(test, 1, 1, batch_size)\n",
    "train_generator = train_loader.get()\n",
    "val_generator = validation_loader.get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization Model (GMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderNet(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_prods, embedding_size, **kwargs):\n",
    "        super(RecommenderNet, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_prods = num_prods\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = tf.keras.layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer= tf.keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.user_bias = tf.keras.layers.Embedding(num_users, 1)\n",
    "        self.prod_embedding = tf.keras.layers.Embedding(\n",
    "            num_prods,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer= tf.keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.prod_bias = tf.keras.layers.Embedding(num_prods, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        user_bias = self.user_bias(inputs[:, 0])\n",
    "        prod_vector = self.prod_embedding(inputs[:, 1])\n",
    "        prod_bias = self.prod_bias(inputs[:, 1])\n",
    "        dot_user_prod = tf.tensordot(user_vector, prod_vector, 2)\n",
    "        x = dot_user_prod + user_bias + prod_bias\n",
    "\n",
    "        return tf.nn.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecommenderNet(number_of_customer, number_of_products, 128)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, epochs=1, steps_per_epoch= steps_per_epoch // batch_size, validation_data = val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s9/93b6jyx57x33y6tt142gsgjc0000gn/T/ipykernel_11023/1139675545.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad61a90f8b8ec2120f8b8d3efc4267caee6017c89f89db236bf61b71644f2c7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
