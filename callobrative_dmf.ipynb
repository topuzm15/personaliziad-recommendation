{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "from dask import dataframe as dd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = dd.read_csv('data/transactions_train.csv')[[\"customer_id\", \"article_id\"]]\n",
    "customer_purchase_number = transactions.groupby(\"customer_id\").size().to_frame(\"prod_number\").reset_index()\n",
    "transactions = transactions.merge(customer_purchase_number, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "c_ids = transactions.customer_id.unique()\n",
    "number_of_customer = len(c_ids)\n",
    "customer_encoding = {c_id: i for i, c_id in enumerate(c_ids)}\n",
    "p_ids = transactions.article_id.unique()\n",
    "number_of_products = len(p_ids)\n",
    "product_encoding = {p_id: i for i, p_id in enumerate(p_ids)}\n",
    "with open('model_data/customer_id_encoding.json', 'w') as fp:\n",
    "    json.dump(customer_encoding, fp)\n",
    "\n",
    "with open('model_data/product_id_encoding.json', 'w') as fp:\n",
    "    json.dump(product_encoding, fp)\n",
    "\n",
    "transactions.customer_id = transactions.customer_id.map(customer_encoding)\n",
    "transactions.article_id = transactions.article_id.map(product_encoding)\n",
    "p_ids = list(product_encoding.values())\n",
    "del c_ids, product_encoding, customer_encoding, customer_purchase_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = transactions.random_split([0.9, 0.1], random_state=43)\n",
    "df = test.merge(train[[\"customer_id\"]], on=[\"customer_id\"], how=\"outer\", indicator=True)\n",
    "train = dd.concat([train, df[df['_merge'] == 'left_only'][[\"customer_id\", \"article_id\"]]], axis=0, ignore_index=True, interleave_partitions=True, ignore_order=True)\n",
    "del transactions, df, test\n",
    "train = train.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(transactions, p_ids):\n",
    "    transactions = transactions.groupby([\"customer_id\"])['article_id']\\\n",
    "                                .apply(lambda x: list(x), meta=(\"article_ids\",object))\\\n",
    "                                .reset_index().compute().drop_duplicates(subset=[\"customer_id\"])\n",
    "\n",
    "    transactions[\"hist_len\"] = transactions.article_ids.apply(lambda x: 128 if len(x)>128 else len(x))\n",
    "    transactions[\"prod_ids\"] = transactions.apply(lambda x: random.sample(x.article_ids, x.hist_len), axis=1)\n",
    "    transactions[\"not_prods\"] = transactions.prod_ids.apply(lambda x: [p_id for p_id in random.sample(p_ids, 256) if p_id not in x][:128])\n",
    "\n",
    "    return transactions[[\"customer_id\", \"prod_ids\", \"not_prods\"]]\n",
    "\n",
    "train = prepare_data(dd.from_pandas(train, npartitions=4), p_ids)\n",
    "train.to_pickle('data/train.pkl')\n",
    "del p_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, positive_sample_length, negative_sample_length, batch_size):\n",
    "        self.data = data\n",
    "        self.positive_sample_length = positive_sample_length\n",
    "        self.negative_sample_length = negative_sample_length\n",
    "        self.sample_length = positive_sample_length + negative_sample_length\n",
    "        self.batch_size = batch_size\n",
    "        if batch_size % self.sample_length != 0:\n",
    "            raise ValueError(\"batch_size must be divisible by sum of positive_sample_length and negative_sample_length\")\n",
    "\n",
    "\n",
    "    def user_info_generator(self):\n",
    "        for i, row in self.data[[\"customer_id\",\"prod_ids\",\"not_prods\"]].iterrows():\n",
    "            pids = np.asarray(random.sample(row[\"prod_ids\"], self.positive_sample_length) + random.sample(row[\"not_prods\"], self.negative_sample_length))\n",
    "            labels = np.asarray([1]*self.positive_sample_length + [0]*self.negative_sample_length)\n",
    "            indices = np.arange(self.sample_length)\n",
    "            np.random.shuffle(indices)\n",
    "            yield tf.convert_to_tensor(np.asarray([pids[indices],labels[indices]], dtype=np.int32), dtype=tf.int32)\n",
    "\n",
    "    def user_id_generator(self):\n",
    "        for i, row in self.data[[\"customer_id\"]].iterrows():\n",
    "            customer_id = [int(row[\"customer_id\"])] * self.sample_length\n",
    "            yield tf.convert_to_tensor(customer_id, dtype=tf.int32)\n",
    "\n",
    "class BatchGenerator(Generator):\n",
    "    def __init__(self, data, positive_sample_length, negative_sample_length, batch_size):\n",
    "        super().__init__(data, positive_sample_length, negative_sample_length, batch_size)\n",
    "        self.mini_batch = int(batch_size / self.sample_length)\n",
    "        self.batch = batch_size\n",
    "\n",
    "        self.user_info_loader = tf.data.Dataset.from_generator(\n",
    "        self.user_info_generator, output_types=tf.int32).batch(self.mini_batch, drop_remainder=True)\n",
    "        self.user_id_loader = tf.data.Dataset.from_generator(\n",
    "        self.user_id_generator, output_types=tf.int32).batch(self.mini_batch, drop_remainder=True)\n",
    "\n",
    "    def _get_batch(self):\n",
    "        for c_ids, info in zip(self.user_id_loader, self.user_info_loader):\n",
    "            x = tf.stack([tf.reshape(c_ids, self.batch), tf.reshape(info[:,0,:], self.batch)], axis=1)\n",
    "            y = tf.reshape(info[:,1,:], self.batch)\n",
    "            yield x, y\n",
    "\n",
    "class DataGenerator (BatchGenerator):\n",
    "    def __init__(self, data, positive_sample_length, negative_sample_length, batch_size, validation=False):\n",
    "        super().__init__(data, positive_sample_length, negative_sample_length, batch_size)\n",
    "        self.len_data = len(self.data) // self.batch_size\n",
    "        self.validation = validation\n",
    "        if validation:\n",
    "            self.val_data = self.data.copy()\n",
    "            self.data = self.val_data.sample(frac=0.1)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.validation:\n",
    "            self.data = self.val_data.sample(frac=0.1)\n",
    "        else:\n",
    "            shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(iter(self._get_batch()))\n",
    "\n",
    "    def get(self):\n",
    "        return self._get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(\"data/train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_generator = DataGenerator(train_data, 1, 3, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization Model (GMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmfNet(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_prods, embedding_size, **kwargs):\n",
    "        super(GmfNet, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_prods = num_prods\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = tf.keras.layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer= tf.keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.user_bias = tf.keras.layers.Embedding(num_users, 1)\n",
    "        self.prod_embedding = tf.keras.layers.Embedding(\n",
    "            num_prods,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer= tf.keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.prod_bias = tf.keras.layers.Embedding(num_prods, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        user_bias = self.user_bias(inputs[:, 0])\n",
    "        prod_vector = self.prod_embedding(inputs[:, 1])\n",
    "        prod_bias = self.prod_bias(inputs[:, 1])\n",
    "        dot_user_prod = tf.tensordot(user_vector, prod_vector, 2)\n",
    "        x = dot_user_prod + user_bias + prod_bias\n",
    "\n",
    "        return tf.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpNet(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_prods, embedding_size, **kwargs):\n",
    "        super(MlpNet, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_prods = num_prods\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = tf.keras.layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer= tf.keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.prod_embedding = tf.keras.layers.Embedding(\n",
    "            num_prods,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer= tf.keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "\n",
    "        self.prediction = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dropout(0.1),\n",
    "            tf.keras.layers.Dense(embedding_size, activation=\"relu\", name=\"layer1\"),\n",
    "            tf.keras.layers.Dropout(0.1),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(embedding_size, activation=\"relu\", name=\"layer2\"),\n",
    "            tf.keras.layers.Dropout(0.1),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(embedding_size, activation=\"relu\", name=\"layer3\"),\n",
    "            tf.keras.layers.Dense(embedding_size, activation=\"relu\", name=\"layer4\"),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        prod_vector = self.prod_embedding(inputs[:, 1])\n",
    "\n",
    "        return self.prediction(tf.concat([user_vector, prod_vector], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderNet(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_prods, embedding_size, **kwargs):\n",
    "        super(RecommenderNet, self).__init__(**kwargs)\n",
    "        self.gmf = GmfNet(num_users, num_prods, embedding_size)\n",
    "        self.mlp = MlpNet(num_users, num_prods, embedding_size)\n",
    "        self.pred = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        gmf_output = self.gmf(inputs)\n",
    "        mlp_output = self.mlp(inputs)\n",
    "        return self.pred(tf.concat([gmf_output, mlp_output], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;80822a2798a5edc8;/job:localhost/replica:0/task:0/device:GPU:0;edge_60_recommender_net/gmf_net/embedding/embeddings/Regularizer/Square/ReadVariableOp;0:0\n\t [[{{node recommender_net/gmf_net/embedding/embeddings/Regularizer/Square/ReadVariableOp/_12}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Adam/gradients/AddN_6/_102]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;80822a2798a5edc8;/job:localhost/replica:0/task:0/device:GPU:0;edge_60_recommender_net/gmf_net/embedding/embeddings/Regularizer/Square/ReadVariableOp;0:0\n\t [[{{node recommender_net/gmf_net/embedding/embeddings/Regularizer/Square/ReadVariableOp/_12}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2845]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32md:\\workspace\\H-M\\callobrative_dmf.ipynb Cell 14'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=7'>8</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=8'>9</a>\u001b[0m     filepath\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mweihts/collabrative\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=9'>10</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=10'>11</a>\u001b[0m     save_weights_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=11'>12</a>\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=12'>13</a>\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=13'>14</a>\u001b[0m cores \u001b[39m=\u001b[39m multiprocessing\u001b[39m.\u001b[39mcpu_count()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=14'>15</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_generator, epochs\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[es, checkpoint], workers\u001b[39m=\u001b[39;49mcores)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1092'>1093</a>\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1093'>1094</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1094'>1095</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1095'>1096</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1096'>1097</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1097'>1098</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1098'>1099</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1099'>1100</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1100'>1101</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1101'>1102</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/def_function.py?line=825'>826</a>\u001b[0m tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/def_function.py?line=826'>827</a>\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name) \u001b[39mas\u001b[39;00m tm:\n\u001b[1;32m--> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/def_function.py?line=827'>828</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/def_function.py?line=828'>829</a>\u001b[0m   compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experimental_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/def_function.py?line=829'>830</a>\u001b[0m   new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:888\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/def_function.py?line=883'>884</a>\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/def_function.py?line=884'>885</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/def_function.py?line=885'>886</a>\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/def_function.py?line=886'>887</a>\u001b[0m     \u001b[39m# stateless function.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/def_function.py?line=887'>888</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/def_function.py?line=888'>889</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/def_function.py?line=889'>890</a>\u001b[0m   _, _, _, filtered_flat_args \u001b[39m=\u001b[39m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/def_function.py?line=890'>891</a>\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39mcanonicalize_function_inputs(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/def_function.py?line=891'>892</a>\u001b[0m           \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=2938'>2939</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=2939'>2940</a>\u001b[0m   (graph_function,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=2940'>2941</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=2941'>2942</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=2942'>2943</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=1913'>1914</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=1914'>1915</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=1915'>1916</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=1916'>1917</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=1917'>1918</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=1918'>1919</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=1919'>1920</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=1920'>1921</a>\u001b[0m     args,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=1921'>1922</a>\u001b[0m     possible_gradient_type,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=1922'>1923</a>\u001b[0m     executing_eagerly)\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=1923'>1924</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=552'>553</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=553'>554</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=554'>555</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=555'>556</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=556'>557</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=557'>558</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=558'>559</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=559'>560</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=560'>561</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=561'>562</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=562'>563</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=563'>564</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=566'>567</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/function.py?line=567'>568</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/execute.py?line=57'>58</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/execute.py?line=58'>59</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/execute.py?line=59'>60</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/execute.py?line=60'>61</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/execute.py?line=61'>62</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;80822a2798a5edc8;/job:localhost/replica:0/task:0/device:GPU:0;edge_60_recommender_net/gmf_net/embedding/embeddings/Regularizer/Square/ReadVariableOp;0:0\n\t [[{{node recommender_net/gmf_net/embedding/embeddings/Regularizer/Square/ReadVariableOp/_12}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Adam/gradients/AddN_6/_102]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;80822a2798a5edc8;/job:localhost/replica:0/task:0/device:GPU:0;edge_60_recommender_net/gmf_net/embedding/embeddings/Regularizer/Square/ReadVariableOp;0:0\n\t [[{{node recommender_net/gmf_net/embedding/embeddings/Regularizer/Square/ReadVariableOp/_12}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2845]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "#model = RecommenderNet(number_of_customer, c, 128) #1362281 104547\n",
    "model = RecommenderNet(1362281, 104547, 128)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\"accuracy\"])\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"weihts/collabrative\",\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min')\n",
    "cores = multiprocessing.cpu_count()\n",
    "history = model.fit(train_generator, epochs=25, callbacks=[es, checkpoint], workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad61a90f8b8ec2120f8b8d3efc4267caee6017c89f89db236bf61b71644f2c7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
