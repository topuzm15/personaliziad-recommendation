{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "from dask import dataframe as dd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = dd.read_csv('data/transactions_train.csv')[[\"customer_id\", \"article_id\"]]\n",
    "customer_purchase_number = transactions.groupby(\"customer_id\").size().to_frame(\"prod_number\").reset_index()\n",
    "transactions = transactions.merge(customer_purchase_number, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "c_ids = transactions.customer_id.unique()\n",
    "number_of_customer = len(c_ids)\n",
    "customer_encoding = {c_id: i for i, c_id in enumerate(c_ids)}\n",
    "p_ids = transactions.article_id.unique()\n",
    "number_of_products = len(p_ids)\n",
    "product_encoding = {p_id: i for i, p_id in enumerate(p_ids)}\n",
    "with open('model_data/customer_id_encoding.json', 'w') as fp:\n",
    "    json.dump(customer_encoding, fp)\n",
    "\n",
    "with open('model_data/product_id_encoding.json', 'w') as fp:\n",
    "    json.dump(product_encoding, fp)\n",
    "\n",
    "transactions.customer_id = transactions.customer_id.map(customer_encoding)\n",
    "transactions.article_id = transactions.article_id.map(product_encoding)\n",
    "p_ids = list(product_encoding.values())\n",
    "del c_ids, product_encoding, customer_encoding, customer_purchase_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = transactions.random_split([0.9, 0.1], random_state=43)\n",
    "df = test.merge(train[[\"customer_id\"]], on=[\"customer_id\"], how=\"outer\", indicator=True)\n",
    "train = dd.concat([train, df[df['_merge'] == 'left_only'][[\"customer_id\", \"article_id\"]]], axis=0, ignore_index=True, interleave_partitions=True, ignore_order=True)\n",
    "del transactions, df, test\n",
    "train = train.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(transactions, p_ids):\n",
    "    transactions = transactions.groupby([\"customer_id\"])['article_id']\\\n",
    "                                .apply(lambda x: list(x), meta=(\"article_ids\",object))\\\n",
    "                                .reset_index().compute().drop_duplicates(subset=[\"customer_id\"])\n",
    "\n",
    "    transactions[\"hist_len\"] = transactions.article_ids.apply(lambda x: 128 if len(x)>128 else len(x))\n",
    "    transactions[\"prod_ids\"] = transactions.apply(lambda x: random.sample(x.article_ids, x.hist_len), axis=1)\n",
    "    transactions[\"not_prods\"] = transactions.prod_ids.apply(lambda x: [p_id for p_id in random.sample(p_ids, 256) if p_id not in x][:128])\n",
    "\n",
    "    return transactions[[\"customer_id\", \"prod_ids\", \"not_prods\"]]\n",
    "\n",
    "train = prepare_data(dd.from_pandas(train, npartitions=4), p_ids)\n",
    "train.to_pickle('data/train.pkl')\n",
    "del p_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, positive_sample_length, negative_sample_length, batch_size):\n",
    "        self.data = data\n",
    "        self.positive_sample_length = positive_sample_length\n",
    "        self.negative_sample_length = negative_sample_length\n",
    "        self.sample_length = positive_sample_length + negative_sample_length\n",
    "        self.batch_size = batch_size\n",
    "        if batch_size % self.sample_length != 0:\n",
    "            raise ValueError(\"batch_size must be divisible by sum of positive_sample_length and negative_sample_length\")\n",
    "\n",
    "\n",
    "    def user_info_generator(self):\n",
    "        for i, row in self.data[[\"customer_id\",\"prod_ids\",\"not_prods\"]].iterrows():\n",
    "            pids = np.asarray(random.sample(row[\"prod_ids\"], self.positive_sample_length) + random.sample(row[\"not_prods\"], self.negative_sample_length))\n",
    "            labels = np.asarray([1]*self.positive_sample_length + [0]*self.negative_sample_length)\n",
    "            indices = np.arange(self.sample_length)\n",
    "            np.random.shuffle(indices)\n",
    "            yield tf.convert_to_tensor(np.asarray([pids[indices],labels[indices]], dtype=np.int32), dtype=tf.int32)\n",
    "\n",
    "    def user_id_generator(self):\n",
    "        for i, row in self.data[[\"customer_id\"]].iterrows():\n",
    "            customer_id = [int(row[\"customer_id\"])] * self.sample_length\n",
    "            yield tf.convert_to_tensor(customer_id, dtype=tf.int32)\n",
    "\n",
    "class BatchGenerator(Generator):\n",
    "    def __init__(self, data, positive_sample_length, negative_sample_length, batch_size):\n",
    "        super().__init__(data, positive_sample_length, negative_sample_length, batch_size)\n",
    "        self.mini_batch = int(batch_size / self.sample_length)\n",
    "        self.batch = batch_size\n",
    "\n",
    "        self.user_info_loader = tf.data.Dataset.from_generator(\n",
    "        self.user_info_generator, output_types=tf.int32).batch(self.mini_batch, drop_remainder=True)\n",
    "        self.user_id_loader = tf.data.Dataset.from_generator(\n",
    "        self.user_id_generator, output_types=tf.int32).batch(self.mini_batch, drop_remainder=True)\n",
    "\n",
    "    def _get_batch(self):\n",
    "        for c_ids, info in zip(self.user_id_loader, self.user_info_loader):\n",
    "            x = tf.stack([tf.reshape(c_ids, self.batch), tf.reshape(info[:,0,:], self.batch)], axis=1)\n",
    "            y = tf.reshape(info[:,1,:], self.batch)\n",
    "            yield x, y\n",
    "\n",
    "class DataGenerator (BatchGenerator):\n",
    "    def __init__(self, data, positive_sample_length, negative_sample_length, batch_size, validation=False):\n",
    "        super().__init__(data, positive_sample_length, negative_sample_length, batch_size)\n",
    "        self.len_data = len(self.data) // self.batch_size\n",
    "        self.validation = validation\n",
    "        if validation:\n",
    "            self.val_data = self.data.copy()\n",
    "            self.data = self.val_data.sample(frac=0.1)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.validation:\n",
    "            self.data = self.val_data.sample(frac=0.1)\n",
    "        else:\n",
    "            shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(iter(self._get_batch()))\n",
    "\n",
    "    def get(self):\n",
    "        return self._get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(\"data/train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>prod_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00018385675844f7a6babbed41b5655b5727fb16483b6e...</td>\n",
       "      <td>[621020001.0, 667916002.0, 651273004.0, 616849...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002db27a1651998a3de4463437b580b45dfa7d8107afa...</td>\n",
       "      <td>[688873010.0, 775328001.0, 615092002.0, 573716...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000cc20cbe16277f5b83992a7d62d2b0ed38389bb0d6fb...</td>\n",
       "      <td>[619506006.0, 671640001.0, 757748003.0, 297067...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0014669814e09bf5ab74d7d4d6befa94f303ff2a04b9f6...</td>\n",
       "      <td>[862496001.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00177f8e34c9496cea9b9f9387842b4e2d050508fbf98d...</td>\n",
       "      <td>[508932016.0, 705691001.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  \\\n",
       "0  00018385675844f7a6babbed41b5655b5727fb16483b6e...   \n",
       "1  0002db27a1651998a3de4463437b580b45dfa7d8107afa...   \n",
       "2  000cc20cbe16277f5b83992a7d62d2b0ed38389bb0d6fb...   \n",
       "3  0014669814e09bf5ab74d7d4d6befa94f303ff2a04b9f6...   \n",
       "4  00177f8e34c9496cea9b9f9387842b4e2d050508fbf98d...   \n",
       "\n",
       "                                            prod_ids  \n",
       "0  [621020001.0, 667916002.0, 651273004.0, 616849...  \n",
       "1  [688873010.0, 775328001.0, 615092002.0, 573716...  \n",
       "2  [619506006.0, 671640001.0, 757748003.0, 297067...  \n",
       "3                                      [862496001.0]  \n",
       "4                         [508932016.0, 705691001.0]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_generator = DataGenerator(train_data, 1, 3, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization Model (GMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmfNet(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_prods, embedding_size, **kwargs):\n",
    "        super(GmfNet, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_prods = num_prods\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = tf.keras.layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer= tf.keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.user_bias = tf.keras.layers.Embedding(num_users, 1)\n",
    "        self.prod_embedding = tf.keras.layers.Embedding(\n",
    "            num_prods,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer= tf.keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.prod_bias = tf.keras.layers.Embedding(num_prods, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        user_bias = self.user_bias(inputs[:, 0])\n",
    "        prod_vector = self.prod_embedding(inputs[:, 1])\n",
    "        prod_bias = self.prod_bias(inputs[:, 1])\n",
    "        dot_user_prod = tf.tensordot(user_vector, prod_vector, 2)\n",
    "        x = dot_user_prod + user_bias + prod_bias\n",
    "\n",
    "        return tf.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpNet(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_prods, embedding_size, **kwargs):\n",
    "        super(MlpNet, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_prods = num_prods\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = tf.keras.layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer= tf.keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.prod_embedding = tf.keras.layers.Embedding(\n",
    "            num_prods,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer= tf.keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "\n",
    "        self.prediction = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dropout(0.1),\n",
    "            tf.keras.layers.Dense(embedding_size, activation=\"relu\", name=\"layer1\"),\n",
    "            tf.keras.layers.Dropout(0.1),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(embedding_size, activation=\"relu\", name=\"layer2\"),\n",
    "            tf.keras.layers.Dropout(0.1),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(embedding_size, activation=\"relu\", name=\"layer3\"),\n",
    "            tf.keras.layers.Dense(embedding_size, activation=\"relu\", name=\"layer4\"),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        prod_vector = self.prod_embedding(inputs[:, 1])\n",
    "\n",
    "        return self.prediction(tf.concat([user_vector, prod_vector], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderNet(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_prods, embedding_size, **kwargs):\n",
    "        super(RecommenderNet, self).__init__(**kwargs)\n",
    "        self.gmf = GmfNet(num_users, num_prods, embedding_size)\n",
    "        self.mlp = MlpNet(num_users, num_prods, embedding_size)\n",
    "        self.pred = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        gmf_output = self.gmf(inputs)\n",
    "        mlp_output = self.mlp(inputs)\n",
    "        return self.pred(tf.concat([gmf_output, mlp_output], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "ValueError: invalid literal for int() with base 10: '00018385675844f7a6babbed41b5655b5727fb16483b6ea51d5798a6ab947344'\nTraceback (most recent call last):\n\n  File \"C:\\Users\\topuz\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 247, in __call__\n    return func(device, token, args)\n\n  File \"C:\\Users\\topuz\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 135, in __call__\n    ret = self._func(*args)\n\n  File \"C:\\Users\\topuz\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 620, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\topuz\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 960, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id.numpy()))\n\n  File \"C:\\Users\\topuz\\AppData\\Local\\Temp\\ipykernel_28664\\755923806.py\", line 22, in user_id_generator\n    customer_id = [int(row[\"customer_id\"])] * self.sample_length\n\nValueError: invalid literal for int() with base 10: '00018385675844f7a6babbed41b5655b5727fb16483b6ea51d5798a6ab947344'\n\n\n\t [[{{node EagerPyFunc}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:2113\u001b[0m, in \u001b[0;36mexecution_mode\u001b[1;34m(mode)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/context.py?line=2111'>2112</a>\u001b[0m   ctx\u001b[39m.\u001b[39mexecutor \u001b[39m=\u001b[39m executor_new\n\u001b[1;32m-> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/context.py?line=2112'>2113</a>\u001b[0m   \u001b[39myield\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/context.py?line=2113'>2114</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:730\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=728'>729</a>\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[1;32m--> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=729'>730</a>\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=730'>731</a>\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=731'>732</a>\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=732'>733</a>\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=734'>735</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=735'>736</a>\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:2578\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=2576'>2577</a>\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=2577'>2578</a>\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=2578'>2579</a>\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6862\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/framework/ops.py?line=6860'>6861</a>\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/framework/ops.py?line=6861'>6862</a>\u001b[0m six\u001b[39m.\u001b[39;49mraise_from(core\u001b[39m.\u001b[39;49m_status_to_exception(e\u001b[39m.\u001b[39;49mcode, message), \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: ValueError: invalid literal for int() with base 10: '00018385675844f7a6babbed41b5655b5727fb16483b6ea51d5798a6ab947344'\nTraceback (most recent call last):\n\n  File \"C:\\Users\\topuz\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 247, in __call__\n    return func(device, token, args)\n\n  File \"C:\\Users\\topuz\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 135, in __call__\n    ret = self._func(*args)\n\n  File \"C:\\Users\\topuz\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 620, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\topuz\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 960, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id.numpy()))\n\n  File \"C:\\Users\\topuz\\AppData\\Local\\Temp\\ipykernel_28664\\755923806.py\", line 22, in user_id_generator\n    customer_id = [int(row[\"customer_id\"])] * self.sample_length\n\nValueError: invalid literal for int() with base 10: '00018385675844f7a6babbed41b5655b5727fb16483b6ea51d5798a6ab947344'\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32md:\\workspace\\H-M\\callobrative_dmf.ipynb Cell 14'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=7'>8</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=8'>9</a>\u001b[0m     filepath\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mweihts/collabrative\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=9'>10</a>\u001b[0m     save_weights_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=10'>11</a>\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=11'>12</a>\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=12'>13</a>\u001b[0m     save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=13'>14</a>\u001b[0m cores \u001b[39m=\u001b[39m multiprocessing\u001b[39m.\u001b[39mcpu_count()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000013?line=14'>15</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_generator, epochs\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[es, checkpoint], workers\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1050\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1043'>1044</a>\u001b[0m   val_x, val_y, val_sample_weight \u001b[39m=\u001b[39m (\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1044'>1045</a>\u001b[0m       data_adapter\u001b[39m.\u001b[39munpack_x_y_sample_weight(validation_data))\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1046'>1047</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39mscope(), \\\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1047'>1048</a>\u001b[0m      training_utils\u001b[39m.\u001b[39mRespectCompiledTrainableState(\u001b[39mself\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1048'>1049</a>\u001b[0m   \u001b[39m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1049'>1050</a>\u001b[0m   data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mDataHandler(\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1050'>1051</a>\u001b[0m       x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1051'>1052</a>\u001b[0m       y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1052'>1053</a>\u001b[0m       sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1053'>1054</a>\u001b[0m       batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1054'>1055</a>\u001b[0m       steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1055'>1056</a>\u001b[0m       initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1056'>1057</a>\u001b[0m       epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1057'>1058</a>\u001b[0m       shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1058'>1059</a>\u001b[0m       class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1059'>1060</a>\u001b[0m       max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1060'>1061</a>\u001b[0m       workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1061'>1062</a>\u001b[0m       use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1062'>1063</a>\u001b[0m       model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1063'>1064</a>\u001b[0m       steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution)\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1065'>1066</a>\u001b[0m   \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/training.py?line=1066'>1067</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1100\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1096'>1097</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution_value \u001b[39m=\u001b[39m steps_per_execution\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1098'>1099</a>\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1099'>1100</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1100'>1101</a>\u001b[0m     x,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1101'>1102</a>\u001b[0m     y,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1102'>1103</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1103'>1104</a>\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1104'>1105</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1105'>1106</a>\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1106'>1107</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1107'>1108</a>\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1108'>1109</a>\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1109'>1110</a>\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1110'>1111</a>\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mds_context\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1111'>1112</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1113'>1114</a>\u001b[0m strategy \u001b[39m=\u001b[39m ds_context\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1114'>1115</a>\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter\u001b[39m.\u001b[39mget_dataset()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:902\u001b[0m, in \u001b[0;36mKerasSequenceAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=899'>900</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keras_sequence \u001b[39m=\u001b[39m x\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=900'>901</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enqueuer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=901'>902</a>\u001b[0m \u001b[39msuper\u001b[39;49m(KerasSequenceAdapter, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=902'>903</a>\u001b[0m     x,\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=903'>904</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# Shuffle is handed in the _make_callable override.\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=904'>905</a>\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=905'>906</a>\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=906'>907</a>\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=907'>908</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=908'>909</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:779\u001b[0m, in \u001b[0;36mGeneratorDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=774'>775</a>\u001b[0m \u001b[39msuper\u001b[39m(GeneratorDataAdapter, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(x, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=776'>777</a>\u001b[0m \u001b[39m# Since we have to know the dtype of the python generator when we build the\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=777'>778</a>\u001b[0m \u001b[39m# dataset, we have to look at a batch to infer the structure.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=778'>779</a>\u001b[0m peek, x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_peek_and_restore(x)\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=779'>780</a>\u001b[0m peek \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_standardize_batch(peek)\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=780'>781</a>\u001b[0m peek \u001b[39m=\u001b[39m _process_tensorlike(peek)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:913\u001b[0m, in \u001b[0;36mKerasSequenceAdapter._peek_and_restore\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=910'>911</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=911'>912</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_peek_and_restore\u001b[39m(x):\n\u001b[1;32m--> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=912'>913</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m x[\u001b[39m0\u001b[39;49m], x\n",
      "\u001b[1;32md:\\workspace\\H-M\\callobrative_dmf.ipynb Cell 7'\u001b[0m in \u001b[0;36mDataGenerator.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000006?line=59'>60</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000006?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_batch()))\n",
      "\u001b[1;32md:\\workspace\\H-M\\callobrative_dmf.ipynb Cell 7'\u001b[0m in \u001b[0;36mBatchGenerator._get_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000006?line=35'>36</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_batch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000006?line=36'>37</a>\u001b[0m     \u001b[39mfor\u001b[39;00m c_ids, info \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_id_loader, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_info_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000006?line=37'>38</a>\u001b[0m         x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mstack([tf\u001b[39m.\u001b[39mreshape(c_ids, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch), tf\u001b[39m.\u001b[39mreshape(info[:,\u001b[39m0\u001b[39m,:], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch)], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/H-M/callobrative_dmf.ipynb#ch0000006?line=38'>39</a>\u001b[0m         y \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(info[:,\u001b[39m1\u001b[39m,:], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:747\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=744'>745</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=745'>746</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=746'>747</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=747'>748</a>\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=748'>749</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:739\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=736'>737</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=737'>738</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=738'>739</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m structure\u001b[39m.\u001b[39mfrom_compatible_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec, ret)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\contextlib.py:131\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/contextlib.py?line=128'>129</a>\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m()\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/contextlib.py?line=129'>130</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/contextlib.py?line=130'>131</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(\u001b[39mtype\u001b[39;49m, value, traceback)\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/contextlib.py?line=131'>132</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/contextlib.py?line=132'>133</a>\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/contextlib.py?line=133'>134</a>\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/contextlib.py?line=134'>135</a>\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/contextlib.py?line=135'>136</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m value\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:2116\u001b[0m, in \u001b[0;36mexecution_mode\u001b[1;34m(mode)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/context.py?line=2113'>2114</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/context.py?line=2114'>2115</a>\u001b[0m   ctx\u001b[39m.\u001b[39mexecutor \u001b[39m=\u001b[39m executor_old\n\u001b[1;32m-> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/context.py?line=2115'>2116</a>\u001b[0m   executor_new\u001b[39m.\u001b[39;49mwait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\executor.py:69\u001b[0m, in \u001b[0;36mExecutor.wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/executor.py?line=66'>67</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/executor.py?line=67'>68</a>\u001b[0m   \u001b[39m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/topuz/anaconda3/envs/tf/lib/site-packages/tensorflow/python/eager/executor.py?line=68'>69</a>\u001b[0m   pywrap_tfe\u001b[39m.\u001b[39;49mTFE_ExecutorWaitForAllPendingNodes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: ValueError: invalid literal for int() with base 10: '00018385675844f7a6babbed41b5655b5727fb16483b6ea51d5798a6ab947344'\nTraceback (most recent call last):\n\n  File \"C:\\Users\\topuz\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 247, in __call__\n    return func(device, token, args)\n\n  File \"C:\\Users\\topuz\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 135, in __call__\n    ret = self._func(*args)\n\n  File \"C:\\Users\\topuz\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 620, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\topuz\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 960, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id.numpy()))\n\n  File \"C:\\Users\\topuz\\AppData\\Local\\Temp\\ipykernel_28664\\755923806.py\", line 22, in user_id_generator\n    customer_id = [int(row[\"customer_id\"])] * self.sample_length\n\nValueError: invalid literal for int() with base 10: '00018385675844f7a6babbed41b5655b5727fb16483b6ea51d5798a6ab947344'\n\n\n\t [[{{node EagerPyFunc}}]]"
     ]
    }
   ],
   "source": [
    "#model = RecommenderNet(number_of_customer, number_of_products, 128) #1007640 104410\n",
    "model = RecommenderNet(1007640, 104410, 128)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\"accuracy\"])\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"weihts/collabrative\",\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "cores = multiprocessing.cpu_count()\n",
    "history = model.fit(train_generator, epochs=25, callbacks=[es, checkpoint], workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad61a90f8b8ec2120f8b8d3efc4267caee6017c89f89db236bf61b71644f2c7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
